{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4823c8a4-35d6-4e38-93d6-1f80bbdb3e20",
   "metadata": {},
   "source": [
    "## Check .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facfb55a-0442-44d5-865b-f79c8c100aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ æª”æ¡ˆåç¨±: sample_submission.csv\n",
      "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
      "0  00001cee341fdb12    0.5           0.5      0.5     0.5     0.5   \n",
      "1  0000247867823ef7    0.5           0.5      0.5     0.5     0.5   \n",
      "2  00013b17ad220c46    0.5           0.5      0.5     0.5     0.5   \n",
      "3  00017563c3f7919a    0.5           0.5      0.5     0.5     0.5   \n",
      "4  00017695ad8997eb    0.5           0.5      0.5     0.5     0.5   \n",
      "\n",
      "   identity_hate  \n",
      "0            0.5  \n",
      "1            0.5  \n",
      "2            0.5  \n",
      "3            0.5  \n",
      "4            0.5  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ æª”æ¡ˆåç¨±: test.csv\n",
      "                 id                                       comment_text\n",
      "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
      "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
      "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
      "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
      "4  00017695ad8997eb          I don't anonymously edit articles at all.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ æª”æ¡ˆåç¨±: test_labels.csv\n",
      "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
      "0  00001cee341fdb12     -1            -1       -1      -1      -1   \n",
      "1  0000247867823ef7     -1            -1       -1      -1      -1   \n",
      "2  00013b17ad220c46     -1            -1       -1      -1      -1   \n",
      "3  00017563c3f7919a     -1            -1       -1      -1      -1   \n",
      "4  00017695ad8997eb     -1            -1       -1      -1      -1   \n",
      "\n",
      "   identity_hate  \n",
      "0             -1  \n",
      "1             -1  \n",
      "2             -1  \n",
      "3             -1  \n",
      "4             -1  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ æª”æ¡ˆåç¨±: train.csv\n",
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# è¨­å®šä½ çš„è³‡æ–™å¤¾è·¯å¾‘\n",
    "data_dir = '.'\n",
    "\n",
    "# éæ­·è³‡æ–™å¤¾ä¸­æ‰€æœ‰ csv æª”æ¡ˆ\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        print(f\"ğŸ“„ æª”æ¡ˆåç¨±: {filename}\")\n",
    "        df = pd.read_csv(filepath, nrows=5)  # åªè®€å–å‰5è¡Œ\n",
    "        print(df)\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160404d9-e68b-463e-a317-7d4eb8838ad9",
   "metadata": {},
   "source": [
    "## Install module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc7aa18-f7d2-4a42-af04-48a3d4248c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (4.52.4)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/10.7 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.7/10.7 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.6/10.7 MB 13.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.2/10.7 MB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 13.1 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.0-cp312-cp312-win_amd64.whl (38.4 MB)\n",
      "   ---------------------------------------- 0.0/38.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.4/38.4 MB 15.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 6.8/38.4 MB 16.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 10.0/38.4 MB 15.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 13.6/38.4 MB 16.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 17.0/38.4 MB 16.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 20.4/38.4 MB 15.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 23.1/38.4 MB 15.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 26.2/38.4 MB 15.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.4/38.4 MB 15.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.5/38.4 MB 15.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.4/38.4 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.4 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.4/38.4 MB 14.8 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.16.0 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a5482-ca87-4e98-b0f5-ea6e035c88e3",
   "metadata": {},
   "source": [
    "## Prepare Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a966e075-207f-4353-80fb-205c036d3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class ToxicCommentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer_name='bert-base-uncased', max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # [max_len]\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),  # [max_len]\n",
    "            'labels': torch.tensor(label, dtype=torch.float)  # [6]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f059c0a-9a0a-4be0-8f09-60d366f34da9",
   "metadata": {},
   "source": [
    "## Create Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ed047ec-f40a-41ed-a30e-f718d742f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class ToxicCommentClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, 6)  # å…­å€‹é¡åˆ¥\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.pooler_output  # [batch_size, hidden_size]\n",
    "        x = self.dropout(pooled)\n",
    "        return self.out(x)  # ä¸åŠ  sigmoidï¼Œç›´æ¥ç”¨ BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17897ee-fb20-436d-90d4-ffab0cf9f555",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e3657b9-0559-474c-8f3e-6adc84b4deea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# è®€å–è³‡æ–™\n",
    "df = pd.read_csv(\".\\\\train.csv\")\n",
    "X = df['comment_text'].fillna(\"\")\n",
    "y = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
    "\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "\n",
    "# è³‡æ–™åˆ‡åˆ†\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# å»ºç«‹ Dataset / DataLoader\n",
    "train_dataset = ToxicCommentDataset(X_train, y_train)\n",
    "val_dataset = ToxicCommentDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9850c00-cc21-4431-9ae4-19ec3d4f18f6",
   "metadata": {},
   "source": [
    "## Define Training Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7587661-3099-4e42-8488-76f37ad74f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 1ï¸âƒ£ è¨­å®šè£ç½®\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2ï¸âƒ£ å»ºç«‹æ¨¡å‹\n",
    "model = ToxicCommentClassifier().to(device)\n",
    "\n",
    "# 3ï¸âƒ£ æå¤±èˆ‡å„ªåŒ–å™¨\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 4ï¸âƒ£ è¨“ç·´å‡½æ•¸\n",
    "def train_epoch(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 5ï¸âƒ£ é©—è­‰å‡½æ•¸\n",
    "def eval_epoch(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16985b8-bb37-459a-9eb6-0cce4326cf2f",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03ab2e50-4d21-46c2-b1e9-a8ccc8fbca83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0478\n",
      "Val   Loss: 0.0395\n",
      "âœ… Saved best model.\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0341\n",
      "Val   Loss: 0.0393\n",
      "âœ… Saved best model.\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0270\n",
      "Val   Loss: 0.0425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "EPOCHS = 3  # ä½ å¯ä»¥èª¿é«˜\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    train_loss = train_epoch(model, train_loader)\n",
    "    val_loss = eval_epoch(model, val_loader)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # å¦‚æœéœ€è¦ï¼Œå¯ä»¥å„²å­˜æ¨¡å‹\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"âœ… Saved best model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a015f-aaf7-4579-894b-03cad7a720a4",
   "metadata": {},
   "source": [
    "## Predict submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8932093e-beb8-4051-88f1-52974098bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# è®€å…¥æ¸¬è©¦è³‡æ–™\n",
    "test_df = pd.read_csv('.\\\\test.csv')\n",
    "test_texts = test_df['comment_text'].fillna(\"\").reset_index(drop=True)\n",
    "\n",
    "# å»ºç«‹ dataset å’Œ dataloader\n",
    "test_dataset = ToxicCommentDataset(test_texts, labels=[[0]*6]*len(test_texts))  # dummy label\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85cc966e-1731-45a4-84c3-f92c42da7922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9573/9573 [16:37<00:00,  9.59it/s]\n"
     ]
    }
   ],
   "source": [
    "model = ToxicCommentClassifier().to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)  # [B, 6]\n",
    "        preds = torch.sigmoid(outputs)              # è½‰æˆæ©Ÿç‡ [0~1]\n",
    "        all_predictions.append(preds.cpu().numpy())\n",
    "\n",
    "# åˆä½µç‚º numpy array\n",
    "import numpy as np\n",
    "final_preds = np.concatenate(all_predictions, axis=0)  # shape: [num_test_samples, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2dc6e69-bbf4-4dc9-b28b-36e95f2b558c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… submission.csv å·²ç”¢å‡ºï¼Œå¯ä¸Šå‚³åˆ° Kaggleï¼\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.read_csv(\".\\\\sample_submission.csv\")\n",
    "\n",
    "# æ›¿æ›æ¯ä¸€æ¬„é æ¸¬åˆ†æ•¸\n",
    "submission_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] = final_preds\n",
    "\n",
    "# å„²å­˜çµæœ\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"âœ… submission.csv å·²ç”¢å‡ºï¼Œå¯ä¸Šå‚³åˆ° Kaggleï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0d96b-3ee0-4408-b4ad-60533f10bbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a6cb7-79e4-4103-9811-5c35a9fbacfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
