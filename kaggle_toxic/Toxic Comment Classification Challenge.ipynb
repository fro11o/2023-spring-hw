{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4823c8a4-35d6-4e38-93d6-1f80bbdb3e20",
   "metadata": {},
   "source": [
    "## Check .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "facfb55a-0442-44d5-865b-f79c8c100aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 檔案名稱: sample_submission.csv\n",
      "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
      "0  00001cee341fdb12    0.5           0.5      0.5     0.5     0.5   \n",
      "1  0000247867823ef7    0.5           0.5      0.5     0.5     0.5   \n",
      "2  00013b17ad220c46    0.5           0.5      0.5     0.5     0.5   \n",
      "3  00017563c3f7919a    0.5           0.5      0.5     0.5     0.5   \n",
      "4  00017695ad8997eb    0.5           0.5      0.5     0.5     0.5   \n",
      "\n",
      "   identity_hate  \n",
      "0            0.5  \n",
      "1            0.5  \n",
      "2            0.5  \n",
      "3            0.5  \n",
      "4            0.5  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "📄 檔案名稱: test.csv\n",
      "                 id                                       comment_text\n",
      "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
      "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
      "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
      "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
      "4  00017695ad8997eb          I don't anonymously edit articles at all.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "📄 檔案名稱: test_labels.csv\n",
      "                 id  toxic  severe_toxic  obscene  threat  insult  \\\n",
      "0  00001cee341fdb12     -1            -1       -1      -1      -1   \n",
      "1  0000247867823ef7     -1            -1       -1      -1      -1   \n",
      "2  00013b17ad220c46     -1            -1       -1      -1      -1   \n",
      "3  00017563c3f7919a     -1            -1       -1      -1      -1   \n",
      "4  00017695ad8997eb     -1            -1       -1      -1      -1   \n",
      "\n",
      "   identity_hate  \n",
      "0             -1  \n",
      "1             -1  \n",
      "2             -1  \n",
      "3             -1  \n",
      "4             -1  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "📄 檔案名稱: train.csv\n",
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 設定你的資料夾路徑\n",
    "data_dir = '.'\n",
    "\n",
    "# 遍歷資料夾中所有 csv 檔案\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        print(f\"📄 檔案名稱: {filename}\")\n",
    "        df = pd.read_csv(filepath, nrows=5)  # 只讀取前5行\n",
    "        print(df)\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160404d9-e68b-463e-a317-7d4eb8838ad9",
   "metadata": {},
   "source": [
    "## Install module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc7aa18-f7d2-4a42-af04-48a3d4248c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (4.52.4)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (0.33.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.0-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\fro11o\\venv\\ml2023\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.6/10.7 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.7/10.7 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.6/10.7 MB 13.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.2/10.7 MB 13.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 13.1 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.0-cp312-cp312-win_amd64.whl (38.4 MB)\n",
      "   ---------------------------------------- 0.0/38.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 3.4/38.4 MB 15.5 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 6.8/38.4 MB 16.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 10.0/38.4 MB 15.9 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 13.6/38.4 MB 16.1 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 17.0/38.4 MB 16.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 20.4/38.4 MB 15.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 23.1/38.4 MB 15.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 26.2/38.4 MB 15.5 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 29.4/38.4 MB 15.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 32.5/38.4 MB 15.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.4/38.4 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.4 MB 15.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.4/38.4 MB 14.8 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.16.0 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80a5482-ca87-4e98-b0f5-ea6e035c88e3",
   "metadata": {},
   "source": [
    "## Prepare Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a966e075-207f-4353-80fb-205c036d3b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class ToxicCommentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer_name='bert-base-uncased', max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts.iloc[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # [max_len]\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),  # [max_len]\n",
    "            'labels': torch.tensor(label, dtype=torch.float)  # [6]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f059c0a-9a0a-4be0-8f09-60d366f34da9",
   "metadata": {},
   "source": [
    "## Create Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ed047ec-f40a-41ed-a30e-f718d742f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class ToxicCommentClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, 6)  # 六個類別\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = outputs.pooler_output  # [batch_size, hidden_size]\n",
    "        x = self.dropout(pooled)\n",
    "        return self.out(x)  # 不加 sigmoid，直接用 BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17897ee-fb20-436d-90d4-ffab0cf9f555",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e3657b9-0559-474c-8f3e-6adc84b4deea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 讀取資料\n",
    "df = pd.read_csv(\".\\\\train.csv\")\n",
    "X = df['comment_text'].fillna(\"\")\n",
    "y = df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
    "\n",
    "print(type(X))\n",
    "print(type(y))\n",
    "\n",
    "# 資料切分\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# 建立 Dataset / DataLoader\n",
    "train_dataset = ToxicCommentDataset(X_train, y_train)\n",
    "val_dataset = ToxicCommentDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9850c00-cc21-4431-9ae4-19ec3d4f18f6",
   "metadata": {},
   "source": [
    "## Define Training Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7587661-3099-4e42-8488-76f37ad74f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 1️⃣ 設定裝置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2️⃣ 建立模型\n",
    "model = ToxicCommentClassifier().to(device)\n",
    "\n",
    "# 3️⃣ 損失與優化器\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 4️⃣ 訓練函數\n",
    "def train_epoch(model, dataloader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# 5️⃣ 驗證函數\n",
    "def eval_epoch(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16985b8-bb37-459a-9eb6-0cce4326cf2f",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03ab2e50-4d21-46c2-b1e9-a8ccc8fbca83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0478\n",
      "Val   Loss: 0.0395\n",
      "✅ Saved best model.\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0341\n",
      "Val   Loss: 0.0393\n",
      "✅ Saved best model.\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0270\n",
      "Val   Loss: 0.0425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "EPOCHS = 3  # 你可以調高\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    train_loss = train_epoch(model, train_loader)\n",
    "    val_loss = eval_epoch(model, val_loader)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val   Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # 如果需要，可以儲存模型\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        print(\"✅ Saved best model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a015f-aaf7-4579-894b-03cad7a720a4",
   "metadata": {},
   "source": [
    "## Predict submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8932093e-beb8-4051-88f1-52974098bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 讀入測試資料\n",
    "test_df = pd.read_csv('.\\\\test.csv')\n",
    "test_texts = test_df['comment_text'].fillna(\"\").reset_index(drop=True)\n",
    "\n",
    "# 建立 dataset 和 dataloader\n",
    "test_dataset = ToxicCommentDataset(test_texts, labels=[[0]*6]*len(test_texts))  # dummy label\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85cc966e-1731-45a4-84c3-f92c42da7922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 9573/9573 [16:37<00:00,  9.59it/s]\n"
     ]
    }
   ],
   "source": [
    "model = ToxicCommentClassifier().to(device)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)  # [B, 6]\n",
    "        preds = torch.sigmoid(outputs)              # 轉成機率 [0~1]\n",
    "        all_predictions.append(preds.cpu().numpy())\n",
    "\n",
    "# 合併為 numpy array\n",
    "import numpy as np\n",
    "final_preds = np.concatenate(all_predictions, axis=0)  # shape: [num_test_samples, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2dc6e69-bbf4-4dc9-b28b-36e95f2b558c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ submission.csv 已產出，可上傳到 Kaggle！\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.read_csv(\".\\\\sample_submission.csv\")\n",
    "\n",
    "# 替換每一欄預測分數\n",
    "submission_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] = final_preds\n",
    "\n",
    "# 儲存結果\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"✅ submission.csv 已產出，可上傳到 Kaggle！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e0d96b-3ee0-4408-b4ad-60533f10bbce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a6cb7-79e4-4103-9811-5c35a9fbacfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
