{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481e111f-2d9a-4a3d-8e64-695f5375de97",
   "metadata": {},
   "source": [
    "# Take a view at the data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dbc8db45-60b0-47ee-b91d-8e36fc0e33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "covid_train = pd.read_csv('covid_train.csv')\n",
    "covid_test = pd.read_csv('covid_test.csv')\n",
    "training_feature = covid_train.iloc[:, 35:-1] # the first column is 'id', so we exclude it.\n",
    "training_label = covid_train.iloc[:, [-1]]\n",
    "testing_feature = covid_test.iloc[:, 35:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a18f8ec8-faf3-4cd9-ae8b-9aeb6a3fab82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cli</th>\n",
       "      <th>ili</th>\n",
       "      <th>wnohh_cmnty_cli</th>\n",
       "      <th>wbelief_masking_effective</th>\n",
       "      <th>wbelief_distancing_effective</th>\n",
       "      <th>wcovid_vaccinated_friends</th>\n",
       "      <th>wlarge_event_indoors</th>\n",
       "      <th>wothers_masked_public</th>\n",
       "      <th>wothers_distanced_public</th>\n",
       "      <th>wshop_indoors</th>\n",
       "      <th>...</th>\n",
       "      <th>wothers_masked_public.2</th>\n",
       "      <th>wothers_distanced_public.2</th>\n",
       "      <th>wshop_indoors.2</th>\n",
       "      <th>wrestaurant_indoors.2</th>\n",
       "      <th>wworried_catch_covid.2</th>\n",
       "      <th>hh_cmnty_cli.2</th>\n",
       "      <th>nohh_cmnty_cli.2</th>\n",
       "      <th>wearing_mask_7d.2</th>\n",
       "      <th>public_transit.2</th>\n",
       "      <th>worried_finances.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.509413</td>\n",
       "      <td>1.511169</td>\n",
       "      <td>18.583362</td>\n",
       "      <td>67.252492</td>\n",
       "      <td>69.654468</td>\n",
       "      <td>53.102556</td>\n",
       "      <td>23.938506</td>\n",
       "      <td>13.605188</td>\n",
       "      <td>20.388001</td>\n",
       "      <td>69.709232</td>\n",
       "      <td>...</td>\n",
       "      <td>13.271292</td>\n",
       "      <td>19.271113</td>\n",
       "      <td>69.050180</td>\n",
       "      <td>38.102142</td>\n",
       "      <td>47.130223</td>\n",
       "      <td>22.686202</td>\n",
       "      <td>17.583283</td>\n",
       "      <td>62.925033</td>\n",
       "      <td>2.704414</td>\n",
       "      <td>39.222329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.451798</td>\n",
       "      <td>1.460472</td>\n",
       "      <td>17.684337</td>\n",
       "      <td>66.918635</td>\n",
       "      <td>68.819058</td>\n",
       "      <td>51.890669</td>\n",
       "      <td>24.328334</td>\n",
       "      <td>13.816449</td>\n",
       "      <td>19.048925</td>\n",
       "      <td>68.763482</td>\n",
       "      <td>...</td>\n",
       "      <td>13.983265</td>\n",
       "      <td>19.500509</td>\n",
       "      <td>68.847156</td>\n",
       "      <td>37.338682</td>\n",
       "      <td>46.598421</td>\n",
       "      <td>22.484758</td>\n",
       "      <td>17.219515</td>\n",
       "      <td>62.771641</td>\n",
       "      <td>2.474973</td>\n",
       "      <td>41.209073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.308107</td>\n",
       "      <td>1.366300</td>\n",
       "      <td>17.194312</td>\n",
       "      <td>66.815913</td>\n",
       "      <td>69.784459</td>\n",
       "      <td>51.982612</td>\n",
       "      <td>24.779964</td>\n",
       "      <td>13.271292</td>\n",
       "      <td>19.271113</td>\n",
       "      <td>69.050180</td>\n",
       "      <td>...</td>\n",
       "      <td>14.133690</td>\n",
       "      <td>18.938706</td>\n",
       "      <td>68.694620</td>\n",
       "      <td>37.543537</td>\n",
       "      <td>46.858400</td>\n",
       "      <td>22.506261</td>\n",
       "      <td>17.128204</td>\n",
       "      <td>62.546116</td>\n",
       "      <td>2.569940</td>\n",
       "      <td>39.636816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.406672</td>\n",
       "      <td>1.488543</td>\n",
       "      <td>16.733442</td>\n",
       "      <td>66.490187</td>\n",
       "      <td>69.991432</td>\n",
       "      <td>53.273235</td>\n",
       "      <td>24.102693</td>\n",
       "      <td>13.983265</td>\n",
       "      <td>19.500509</td>\n",
       "      <td>68.847156</td>\n",
       "      <td>...</td>\n",
       "      <td>13.154932</td>\n",
       "      <td>18.698535</td>\n",
       "      <td>69.339191</td>\n",
       "      <td>37.751874</td>\n",
       "      <td>44.633652</td>\n",
       "      <td>22.369951</td>\n",
       "      <td>17.069263</td>\n",
       "      <td>61.517466</td>\n",
       "      <td>2.610086</td>\n",
       "      <td>38.926817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.381060</td>\n",
       "      <td>1.453365</td>\n",
       "      <td>16.580258</td>\n",
       "      <td>66.212707</td>\n",
       "      <td>70.019855</td>\n",
       "      <td>53.691389</td>\n",
       "      <td>24.070876</td>\n",
       "      <td>14.133690</td>\n",
       "      <td>18.938706</td>\n",
       "      <td>68.694620</td>\n",
       "      <td>...</td>\n",
       "      <td>13.621567</td>\n",
       "      <td>18.034980</td>\n",
       "      <td>69.564435</td>\n",
       "      <td>38.341833</td>\n",
       "      <td>44.100299</td>\n",
       "      <td>21.440588</td>\n",
       "      <td>16.207377</td>\n",
       "      <td>60.933647</td>\n",
       "      <td>2.790749</td>\n",
       "      <td>39.840306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>1.145430</td>\n",
       "      <td>1.174613</td>\n",
       "      <td>11.825744</td>\n",
       "      <td>61.187612</td>\n",
       "      <td>64.170912</td>\n",
       "      <td>62.418350</td>\n",
       "      <td>32.685998</td>\n",
       "      <td>3.512590</td>\n",
       "      <td>11.059251</td>\n",
       "      <td>66.760060</td>\n",
       "      <td>...</td>\n",
       "      <td>4.177710</td>\n",
       "      <td>11.949304</td>\n",
       "      <td>67.172136</td>\n",
       "      <td>45.253751</td>\n",
       "      <td>34.677094</td>\n",
       "      <td>15.416655</td>\n",
       "      <td>11.389769</td>\n",
       "      <td>28.867455</td>\n",
       "      <td>6.065089</td>\n",
       "      <td>30.451465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>1.121511</td>\n",
       "      <td>1.150313</td>\n",
       "      <td>11.772172</td>\n",
       "      <td>61.409643</td>\n",
       "      <td>64.281456</td>\n",
       "      <td>63.069265</td>\n",
       "      <td>33.048620</td>\n",
       "      <td>4.056488</td>\n",
       "      <td>11.460707</td>\n",
       "      <td>67.160173</td>\n",
       "      <td>...</td>\n",
       "      <td>4.536844</td>\n",
       "      <td>12.525530</td>\n",
       "      <td>66.725856</td>\n",
       "      <td>44.988865</td>\n",
       "      <td>33.589880</td>\n",
       "      <td>15.755713</td>\n",
       "      <td>11.811906</td>\n",
       "      <td>29.482770</td>\n",
       "      <td>6.216503</td>\n",
       "      <td>31.196935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>1.049163</td>\n",
       "      <td>1.077583</td>\n",
       "      <td>11.764646</td>\n",
       "      <td>62.647227</td>\n",
       "      <td>65.321622</td>\n",
       "      <td>63.701473</td>\n",
       "      <td>32.249664</td>\n",
       "      <td>4.177710</td>\n",
       "      <td>11.949304</td>\n",
       "      <td>67.172136</td>\n",
       "      <td>...</td>\n",
       "      <td>4.483524</td>\n",
       "      <td>13.271182</td>\n",
       "      <td>66.913987</td>\n",
       "      <td>45.316672</td>\n",
       "      <td>33.076780</td>\n",
       "      <td>15.538073</td>\n",
       "      <td>11.435870</td>\n",
       "      <td>28.911782</td>\n",
       "      <td>6.157999</td>\n",
       "      <td>31.492405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>1.210881</td>\n",
       "      <td>1.239045</td>\n",
       "      <td>12.415059</td>\n",
       "      <td>63.646865</td>\n",
       "      <td>64.748765</td>\n",
       "      <td>64.251005</td>\n",
       "      <td>32.275909</td>\n",
       "      <td>4.536844</td>\n",
       "      <td>12.525530</td>\n",
       "      <td>66.725856</td>\n",
       "      <td>...</td>\n",
       "      <td>3.737855</td>\n",
       "      <td>12.185653</td>\n",
       "      <td>68.278056</td>\n",
       "      <td>46.760569</td>\n",
       "      <td>32.359431</td>\n",
       "      <td>15.980730</td>\n",
       "      <td>11.592346</td>\n",
       "      <td>28.151731</td>\n",
       "      <td>5.886028</td>\n",
       "      <td>32.338100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008</th>\n",
       "      <td>1.257293</td>\n",
       "      <td>1.238664</td>\n",
       "      <td>12.270085</td>\n",
       "      <td>63.477501</td>\n",
       "      <td>64.145132</td>\n",
       "      <td>63.603853</td>\n",
       "      <td>32.796014</td>\n",
       "      <td>4.483524</td>\n",
       "      <td>13.271182</td>\n",
       "      <td>66.913987</td>\n",
       "      <td>...</td>\n",
       "      <td>3.346517</td>\n",
       "      <td>12.193053</td>\n",
       "      <td>68.392842</td>\n",
       "      <td>46.636819</td>\n",
       "      <td>31.629827</td>\n",
       "      <td>15.777992</td>\n",
       "      <td>11.321163</td>\n",
       "      <td>27.768290</td>\n",
       "      <td>6.179323</td>\n",
       "      <td>31.861067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3009 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cli       ili  wnohh_cmnty_cli  wbelief_masking_effective  \\\n",
       "0     1.509413  1.511169        18.583362                  67.252492   \n",
       "1     1.451798  1.460472        17.684337                  66.918635   \n",
       "2     1.308107  1.366300        17.194312                  66.815913   \n",
       "3     1.406672  1.488543        16.733442                  66.490187   \n",
       "4     1.381060  1.453365        16.580258                  66.212707   \n",
       "...        ...       ...              ...                        ...   \n",
       "3004  1.145430  1.174613        11.825744                  61.187612   \n",
       "3005  1.121511  1.150313        11.772172                  61.409643   \n",
       "3006  1.049163  1.077583        11.764646                  62.647227   \n",
       "3007  1.210881  1.239045        12.415059                  63.646865   \n",
       "3008  1.257293  1.238664        12.270085                  63.477501   \n",
       "\n",
       "      wbelief_distancing_effective  wcovid_vaccinated_friends  \\\n",
       "0                        69.654468                  53.102556   \n",
       "1                        68.819058                  51.890669   \n",
       "2                        69.784459                  51.982612   \n",
       "3                        69.991432                  53.273235   \n",
       "4                        70.019855                  53.691389   \n",
       "...                            ...                        ...   \n",
       "3004                     64.170912                  62.418350   \n",
       "3005                     64.281456                  63.069265   \n",
       "3006                     65.321622                  63.701473   \n",
       "3007                     64.748765                  64.251005   \n",
       "3008                     64.145132                  63.603853   \n",
       "\n",
       "      wlarge_event_indoors  wothers_masked_public  wothers_distanced_public  \\\n",
       "0                23.938506              13.605188                 20.388001   \n",
       "1                24.328334              13.816449                 19.048925   \n",
       "2                24.779964              13.271292                 19.271113   \n",
       "3                24.102693              13.983265                 19.500509   \n",
       "4                24.070876              14.133690                 18.938706   \n",
       "...                    ...                    ...                       ...   \n",
       "3004             32.685998               3.512590                 11.059251   \n",
       "3005             33.048620               4.056488                 11.460707   \n",
       "3006             32.249664               4.177710                 11.949304   \n",
       "3007             32.275909               4.536844                 12.525530   \n",
       "3008             32.796014               4.483524                 13.271182   \n",
       "\n",
       "      wshop_indoors  ...  wothers_masked_public.2  wothers_distanced_public.2  \\\n",
       "0         69.709232  ...                13.271292                   19.271113   \n",
       "1         68.763482  ...                13.983265                   19.500509   \n",
       "2         69.050180  ...                14.133690                   18.938706   \n",
       "3         68.847156  ...                13.154932                   18.698535   \n",
       "4         68.694620  ...                13.621567                   18.034980   \n",
       "...             ...  ...                      ...                         ...   \n",
       "3004      66.760060  ...                 4.177710                   11.949304   \n",
       "3005      67.160173  ...                 4.536844                   12.525530   \n",
       "3006      67.172136  ...                 4.483524                   13.271182   \n",
       "3007      66.725856  ...                 3.737855                   12.185653   \n",
       "3008      66.913987  ...                 3.346517                   12.193053   \n",
       "\n",
       "      wshop_indoors.2  wrestaurant_indoors.2  wworried_catch_covid.2  \\\n",
       "0           69.050180              38.102142               47.130223   \n",
       "1           68.847156              37.338682               46.598421   \n",
       "2           68.694620              37.543537               46.858400   \n",
       "3           69.339191              37.751874               44.633652   \n",
       "4           69.564435              38.341833               44.100299   \n",
       "...               ...                    ...                     ...   \n",
       "3004        67.172136              45.253751               34.677094   \n",
       "3005        66.725856              44.988865               33.589880   \n",
       "3006        66.913987              45.316672               33.076780   \n",
       "3007        68.278056              46.760569               32.359431   \n",
       "3008        68.392842              46.636819               31.629827   \n",
       "\n",
       "      hh_cmnty_cli.2  nohh_cmnty_cli.2  wearing_mask_7d.2  public_transit.2  \\\n",
       "0          22.686202         17.583283          62.925033          2.704414   \n",
       "1          22.484758         17.219515          62.771641          2.474973   \n",
       "2          22.506261         17.128204          62.546116          2.569940   \n",
       "3          22.369951         17.069263          61.517466          2.610086   \n",
       "4          21.440588         16.207377          60.933647          2.790749   \n",
       "...              ...               ...                ...               ...   \n",
       "3004       15.416655         11.389769          28.867455          6.065089   \n",
       "3005       15.755713         11.811906          29.482770          6.216503   \n",
       "3006       15.538073         11.435870          28.911782          6.157999   \n",
       "3007       15.980730         11.592346          28.151731          5.886028   \n",
       "3008       15.777992         11.321163          27.768290          6.179323   \n",
       "\n",
       "      worried_finances.2  \n",
       "0              39.222329  \n",
       "1              41.209073  \n",
       "2              39.636816  \n",
       "3              38.926817  \n",
       "4              39.840306  \n",
       "...                  ...  \n",
       "3004           30.451465  \n",
       "3005           31.196935  \n",
       "3006           31.492405  \n",
       "3007           32.338100  \n",
       "3008           31.861067  \n",
       "\n",
       "[3009 rows x 53 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "96c49e88-b196-402d-9e51-96acddcbfafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tested_positive.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.490787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.329253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.522931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.578501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14.171920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>6.487310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>6.112827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>6.151394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>7.165580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008</th>\n",
       "      <td>10.535087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3009 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tested_positive.2\n",
       "0             18.490787\n",
       "1             16.329253\n",
       "2             16.522931\n",
       "3             15.578501\n",
       "4             14.171920\n",
       "...                 ...\n",
       "3004           6.487310\n",
       "3005           6.112827\n",
       "3006           6.151394\n",
       "3007           7.165580\n",
       "3008          10.535087\n",
       "\n",
       "[3009 rows x 1 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "53c8ed17-f263-4fe0-9ff0-9685f78e3ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cli</th>\n",
       "      <th>ili</th>\n",
       "      <th>wnohh_cmnty_cli</th>\n",
       "      <th>wbelief_masking_effective</th>\n",
       "      <th>wbelief_distancing_effective</th>\n",
       "      <th>wcovid_vaccinated_friends</th>\n",
       "      <th>wlarge_event_indoors</th>\n",
       "      <th>wothers_masked_public</th>\n",
       "      <th>wothers_distanced_public</th>\n",
       "      <th>wshop_indoors</th>\n",
       "      <th>...</th>\n",
       "      <th>wothers_masked_public.2</th>\n",
       "      <th>wothers_distanced_public.2</th>\n",
       "      <th>wshop_indoors.2</th>\n",
       "      <th>wrestaurant_indoors.2</th>\n",
       "      <th>wworried_catch_covid.2</th>\n",
       "      <th>hh_cmnty_cli.2</th>\n",
       "      <th>nohh_cmnty_cli.2</th>\n",
       "      <th>wearing_mask_7d.2</th>\n",
       "      <th>public_transit.2</th>\n",
       "      <th>worried_finances.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.969055</td>\n",
       "      <td>4.130866</td>\n",
       "      <td>37.224363</td>\n",
       "      <td>69.847810</td>\n",
       "      <td>70.035264</td>\n",
       "      <td>64.206961</td>\n",
       "      <td>19.131875</td>\n",
       "      <td>15.303902</td>\n",
       "      <td>16.209827</td>\n",
       "      <td>63.011530</td>\n",
       "      <td>...</td>\n",
       "      <td>16.187812</td>\n",
       "      <td>15.996416</td>\n",
       "      <td>63.208478</td>\n",
       "      <td>34.034397</td>\n",
       "      <td>54.787540</td>\n",
       "      <td>42.142738</td>\n",
       "      <td>35.753942</td>\n",
       "      <td>66.459333</td>\n",
       "      <td>3.549976</td>\n",
       "      <td>37.561679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.706909</td>\n",
       "      <td>0.770502</td>\n",
       "      <td>9.321714</td>\n",
       "      <td>75.655733</td>\n",
       "      <td>77.353731</td>\n",
       "      <td>78.404421</td>\n",
       "      <td>22.701887</td>\n",
       "      <td>37.746560</td>\n",
       "      <td>19.854968</td>\n",
       "      <td>64.750537</td>\n",
       "      <td>...</td>\n",
       "      <td>35.507282</td>\n",
       "      <td>17.686027</td>\n",
       "      <td>62.887780</td>\n",
       "      <td>35.300316</td>\n",
       "      <td>49.624786</td>\n",
       "      <td>12.668497</td>\n",
       "      <td>8.902815</td>\n",
       "      <td>68.035555</td>\n",
       "      <td>8.349080</td>\n",
       "      <td>33.921755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.831224</td>\n",
       "      <td>1.868739</td>\n",
       "      <td>21.627214</td>\n",
       "      <td>61.990220</td>\n",
       "      <td>62.847072</td>\n",
       "      <td>58.140973</td>\n",
       "      <td>23.413246</td>\n",
       "      <td>16.544123</td>\n",
       "      <td>16.260507</td>\n",
       "      <td>63.711884</td>\n",
       "      <td>...</td>\n",
       "      <td>16.988814</td>\n",
       "      <td>15.553080</td>\n",
       "      <td>66.314132</td>\n",
       "      <td>36.798828</td>\n",
       "      <td>43.224876</td>\n",
       "      <td>24.278089</td>\n",
       "      <td>19.419730</td>\n",
       "      <td>62.169775</td>\n",
       "      <td>3.609419</td>\n",
       "      <td>36.657118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.099403</td>\n",
       "      <td>2.229969</td>\n",
       "      <td>25.053179</td>\n",
       "      <td>64.461961</td>\n",
       "      <td>66.038925</td>\n",
       "      <td>67.948819</td>\n",
       "      <td>20.233013</td>\n",
       "      <td>33.378497</td>\n",
       "      <td>14.720511</td>\n",
       "      <td>62.357315</td>\n",
       "      <td>...</td>\n",
       "      <td>33.664879</td>\n",
       "      <td>15.302950</td>\n",
       "      <td>63.439616</td>\n",
       "      <td>35.768740</td>\n",
       "      <td>46.545017</td>\n",
       "      <td>29.619471</td>\n",
       "      <td>24.495402</td>\n",
       "      <td>70.137155</td>\n",
       "      <td>4.600004</td>\n",
       "      <td>34.210536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.786286</td>\n",
       "      <td>4.939648</td>\n",
       "      <td>43.258554</td>\n",
       "      <td>64.143980</td>\n",
       "      <td>65.180658</td>\n",
       "      <td>56.466861</td>\n",
       "      <td>20.413600</td>\n",
       "      <td>10.934509</td>\n",
       "      <td>15.173817</td>\n",
       "      <td>57.516071</td>\n",
       "      <td>...</td>\n",
       "      <td>11.893332</td>\n",
       "      <td>15.043238</td>\n",
       "      <td>55.866033</td>\n",
       "      <td>30.800247</td>\n",
       "      <td>50.350116</td>\n",
       "      <td>50.161383</td>\n",
       "      <td>43.380365</td>\n",
       "      <td>62.227135</td>\n",
       "      <td>2.188933</td>\n",
       "      <td>40.253917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>1.477312</td>\n",
       "      <td>1.452640</td>\n",
       "      <td>15.659922</td>\n",
       "      <td>60.146177</td>\n",
       "      <td>66.703123</td>\n",
       "      <td>68.615811</td>\n",
       "      <td>19.546809</td>\n",
       "      <td>17.614679</td>\n",
       "      <td>18.407457</td>\n",
       "      <td>67.989591</td>\n",
       "      <td>...</td>\n",
       "      <td>16.725758</td>\n",
       "      <td>20.019314</td>\n",
       "      <td>68.646911</td>\n",
       "      <td>33.102626</td>\n",
       "      <td>41.383047</td>\n",
       "      <td>18.834134</td>\n",
       "      <td>14.289705</td>\n",
       "      <td>61.266288</td>\n",
       "      <td>4.606961</td>\n",
       "      <td>35.682932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>1.156506</td>\n",
       "      <td>1.171350</td>\n",
       "      <td>16.553513</td>\n",
       "      <td>73.268243</td>\n",
       "      <td>74.281544</td>\n",
       "      <td>67.275138</td>\n",
       "      <td>22.396405</td>\n",
       "      <td>27.865979</td>\n",
       "      <td>22.116621</td>\n",
       "      <td>66.939864</td>\n",
       "      <td>...</td>\n",
       "      <td>27.598087</td>\n",
       "      <td>21.654561</td>\n",
       "      <td>67.108953</td>\n",
       "      <td>34.953507</td>\n",
       "      <td>45.918540</td>\n",
       "      <td>19.720060</td>\n",
       "      <td>15.611472</td>\n",
       "      <td>68.767016</td>\n",
       "      <td>5.010471</td>\n",
       "      <td>34.686565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>2.525665</td>\n",
       "      <td>2.562319</td>\n",
       "      <td>21.930353</td>\n",
       "      <td>62.269028</td>\n",
       "      <td>65.431004</td>\n",
       "      <td>57.545779</td>\n",
       "      <td>23.298426</td>\n",
       "      <td>8.070989</td>\n",
       "      <td>16.930138</td>\n",
       "      <td>65.164466</td>\n",
       "      <td>...</td>\n",
       "      <td>11.705587</td>\n",
       "      <td>18.190111</td>\n",
       "      <td>63.831927</td>\n",
       "      <td>36.732008</td>\n",
       "      <td>50.541024</td>\n",
       "      <td>27.619861</td>\n",
       "      <td>21.168493</td>\n",
       "      <td>54.497223</td>\n",
       "      <td>3.372057</td>\n",
       "      <td>43.279561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>3.457977</td>\n",
       "      <td>3.606956</td>\n",
       "      <td>32.134867</td>\n",
       "      <td>81.233604</td>\n",
       "      <td>79.920255</td>\n",
       "      <td>75.948481</td>\n",
       "      <td>15.130632</td>\n",
       "      <td>60.859562</td>\n",
       "      <td>27.361148</td>\n",
       "      <td>55.706858</td>\n",
       "      <td>...</td>\n",
       "      <td>61.356046</td>\n",
       "      <td>27.113286</td>\n",
       "      <td>55.299278</td>\n",
       "      <td>24.324513</td>\n",
       "      <td>64.569360</td>\n",
       "      <td>41.825075</td>\n",
       "      <td>34.954591</td>\n",
       "      <td>87.193792</td>\n",
       "      <td>5.176875</td>\n",
       "      <td>38.497397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>3.153188</td>\n",
       "      <td>3.228045</td>\n",
       "      <td>29.783871</td>\n",
       "      <td>56.237939</td>\n",
       "      <td>61.357322</td>\n",
       "      <td>51.964458</td>\n",
       "      <td>23.499520</td>\n",
       "      <td>10.486051</td>\n",
       "      <td>14.976743</td>\n",
       "      <td>64.585378</td>\n",
       "      <td>...</td>\n",
       "      <td>10.241117</td>\n",
       "      <td>15.435723</td>\n",
       "      <td>64.409680</td>\n",
       "      <td>34.241837</td>\n",
       "      <td>44.492289</td>\n",
       "      <td>34.594328</td>\n",
       "      <td>28.866970</td>\n",
       "      <td>58.744778</td>\n",
       "      <td>2.495701</td>\n",
       "      <td>36.514362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>997 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          cli       ili  wnohh_cmnty_cli  wbelief_masking_effective  \\\n",
       "0    3.969055  4.130866        37.224363                  69.847810   \n",
       "1    0.706909  0.770502         9.321714                  75.655733   \n",
       "2    1.831224  1.868739        21.627214                  61.990220   \n",
       "3    2.099403  2.229969        25.053179                  64.461961   \n",
       "4    4.786286  4.939648        43.258554                  64.143980   \n",
       "..        ...       ...              ...                        ...   \n",
       "992  1.477312  1.452640        15.659922                  60.146177   \n",
       "993  1.156506  1.171350        16.553513                  73.268243   \n",
       "994  2.525665  2.562319        21.930353                  62.269028   \n",
       "995  3.457977  3.606956        32.134867                  81.233604   \n",
       "996  3.153188  3.228045        29.783871                  56.237939   \n",
       "\n",
       "     wbelief_distancing_effective  wcovid_vaccinated_friends  \\\n",
       "0                       70.035264                  64.206961   \n",
       "1                       77.353731                  78.404421   \n",
       "2                       62.847072                  58.140973   \n",
       "3                       66.038925                  67.948819   \n",
       "4                       65.180658                  56.466861   \n",
       "..                            ...                        ...   \n",
       "992                     66.703123                  68.615811   \n",
       "993                     74.281544                  67.275138   \n",
       "994                     65.431004                  57.545779   \n",
       "995                     79.920255                  75.948481   \n",
       "996                     61.357322                  51.964458   \n",
       "\n",
       "     wlarge_event_indoors  wothers_masked_public  wothers_distanced_public  \\\n",
       "0               19.131875              15.303902                 16.209827   \n",
       "1               22.701887              37.746560                 19.854968   \n",
       "2               23.413246              16.544123                 16.260507   \n",
       "3               20.233013              33.378497                 14.720511   \n",
       "4               20.413600              10.934509                 15.173817   \n",
       "..                    ...                    ...                       ...   \n",
       "992             19.546809              17.614679                 18.407457   \n",
       "993             22.396405              27.865979                 22.116621   \n",
       "994             23.298426               8.070989                 16.930138   \n",
       "995             15.130632              60.859562                 27.361148   \n",
       "996             23.499520              10.486051                 14.976743   \n",
       "\n",
       "     wshop_indoors  ...  wothers_masked_public.2  wothers_distanced_public.2  \\\n",
       "0        63.011530  ...                16.187812                   15.996416   \n",
       "1        64.750537  ...                35.507282                   17.686027   \n",
       "2        63.711884  ...                16.988814                   15.553080   \n",
       "3        62.357315  ...                33.664879                   15.302950   \n",
       "4        57.516071  ...                11.893332                   15.043238   \n",
       "..             ...  ...                      ...                         ...   \n",
       "992      67.989591  ...                16.725758                   20.019314   \n",
       "993      66.939864  ...                27.598087                   21.654561   \n",
       "994      65.164466  ...                11.705587                   18.190111   \n",
       "995      55.706858  ...                61.356046                   27.113286   \n",
       "996      64.585378  ...                10.241117                   15.435723   \n",
       "\n",
       "     wshop_indoors.2  wrestaurant_indoors.2  wworried_catch_covid.2  \\\n",
       "0          63.208478              34.034397               54.787540   \n",
       "1          62.887780              35.300316               49.624786   \n",
       "2          66.314132              36.798828               43.224876   \n",
       "3          63.439616              35.768740               46.545017   \n",
       "4          55.866033              30.800247               50.350116   \n",
       "..               ...                    ...                     ...   \n",
       "992        68.646911              33.102626               41.383047   \n",
       "993        67.108953              34.953507               45.918540   \n",
       "994        63.831927              36.732008               50.541024   \n",
       "995        55.299278              24.324513               64.569360   \n",
       "996        64.409680              34.241837               44.492289   \n",
       "\n",
       "     hh_cmnty_cli.2  nohh_cmnty_cli.2  wearing_mask_7d.2  public_transit.2  \\\n",
       "0         42.142738         35.753942          66.459333          3.549976   \n",
       "1         12.668497          8.902815          68.035555          8.349080   \n",
       "2         24.278089         19.419730          62.169775          3.609419   \n",
       "3         29.619471         24.495402          70.137155          4.600004   \n",
       "4         50.161383         43.380365          62.227135          2.188933   \n",
       "..              ...               ...                ...               ...   \n",
       "992       18.834134         14.289705          61.266288          4.606961   \n",
       "993       19.720060         15.611472          68.767016          5.010471   \n",
       "994       27.619861         21.168493          54.497223          3.372057   \n",
       "995       41.825075         34.954591          87.193792          5.176875   \n",
       "996       34.594328         28.866970          58.744778          2.495701   \n",
       "\n",
       "     worried_finances.2  \n",
       "0             37.561679  \n",
       "1             33.921755  \n",
       "2             36.657118  \n",
       "3             34.210536  \n",
       "4             40.253917  \n",
       "..                  ...  \n",
       "992           35.682932  \n",
       "993           34.686565  \n",
       "994           43.279561  \n",
       "995           38.497397  \n",
       "996           36.514362  \n",
       "\n",
       "[997 rows x 53 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe76c3b-997d-4621-bbb8-f75382005379",
   "metadata": {},
   "source": [
    "## Build pytorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "53d6ff6f-bd4c-47f6-8ae7-9127fab1220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CovidDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.to_numpy(dtype=np.float32)\n",
    "        self.y = y.to_numpy(dtype=np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class RegressionModel_d1(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class RegressionModel_d2(nn.Module):\n",
    "    def __init__(self, input_dim, h1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class RegressionModel_d3(nn.Module):\n",
    "    def __init__(self, input_dim, h1, h2):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, h1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1, h2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, num_epochs, lr=1e-3):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_X, batch_y in dataloader:\n",
    "            preds = model(batch_X).squeeze()  # Make sure shape is (N,)\n",
    "            loss = criterion(preds, batch_y.squeeze())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9028cfd2-1420-470f-9097-66496cb95fb7",
   "metadata": {},
   "source": [
    "## Train d1 Model (public: 0.91479, private: 0.91088, slightly over string baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "79e7d6b4-e8ae-4066-b7db-2f5097eef90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500, Loss: 5.0328\n",
      "Epoch 20/500, Loss: 3.9486\n",
      "Epoch 30/500, Loss: 2.2548\n",
      "Epoch 40/500, Loss: 1.6707\n",
      "Epoch 50/500, Loss: 1.1893\n",
      "Epoch 60/500, Loss: 1.6451\n",
      "Epoch 70/500, Loss: 1.2391\n",
      "Epoch 80/500, Loss: 0.9711\n",
      "Epoch 90/500, Loss: 1.2427\n",
      "Epoch 100/500, Loss: 1.5098\n",
      "Epoch 110/500, Loss: 1.4445\n",
      "Epoch 120/500, Loss: 1.4107\n",
      "Epoch 130/500, Loss: 0.8165\n",
      "Epoch 140/500, Loss: 0.9657\n",
      "Epoch 150/500, Loss: 1.3523\n",
      "Epoch 160/500, Loss: 0.7603\n",
      "Epoch 170/500, Loss: 1.2556\n",
      "Epoch 180/500, Loss: 0.9982\n",
      "Epoch 190/500, Loss: 0.6812\n",
      "Epoch 200/500, Loss: 1.1186\n",
      "Epoch 210/500, Loss: 1.2539\n",
      "Epoch 220/500, Loss: 0.8598\n",
      "Epoch 230/500, Loss: 0.8334\n",
      "Epoch 240/500, Loss: 1.0539\n",
      "Epoch 250/500, Loss: 1.0356\n",
      "Epoch 260/500, Loss: 1.2514\n",
      "Epoch 270/500, Loss: 0.8086\n",
      "Epoch 280/500, Loss: 0.6522\n",
      "Epoch 290/500, Loss: 1.3402\n",
      "Epoch 300/500, Loss: 0.8508\n",
      "Epoch 310/500, Loss: 0.9365\n",
      "Epoch 320/500, Loss: 0.8015\n",
      "Epoch 330/500, Loss: 1.1645\n",
      "Epoch 340/500, Loss: 0.9166\n",
      "Epoch 350/500, Loss: 0.9352\n",
      "Epoch 360/500, Loss: 1.1747\n",
      "Epoch 370/500, Loss: 0.9825\n",
      "Epoch 380/500, Loss: 1.0296\n",
      "Epoch 390/500, Loss: 0.7199\n",
      "Epoch 400/500, Loss: 1.1435\n",
      "Epoch 410/500, Loss: 0.8623\n",
      "Epoch 420/500, Loss: 0.9096\n",
      "Epoch 430/500, Loss: 0.9662\n",
      "Epoch 440/500, Loss: 0.8564\n",
      "Epoch 450/500, Loss: 1.0575\n",
      "Epoch 460/500, Loss: 1.1735\n",
      "Epoch 470/500, Loss: 1.0556\n",
      "Epoch 480/500, Loss: 1.0687\n",
      "Epoch 490/500, Loss: 0.6555\n",
      "Epoch 500/500, Loss: 1.1201\n"
     ]
    }
   ],
   "source": [
    "dataset = CovidDataset(training_feature, training_label)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "model_d1 = RegressionModel_d1(input_dim=87)\n",
    "train_model(model_d1, dataloader, 500, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c5bc91b2-dc3b-4e91-91f6-44b564563d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tested_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>32.603535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.041372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>24.895422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>26.608978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>40.452549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>992</td>\n",
       "      <td>15.621445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>993</td>\n",
       "      <td>13.420916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>994</td>\n",
       "      <td>20.080273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>21.410704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>34.176884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>997 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  tested_positive\n",
       "0      0        32.603535\n",
       "1      1         6.041372\n",
       "2      2        24.895422\n",
       "3      3        26.608978\n",
       "4      4        40.452549\n",
       "..   ...              ...\n",
       "992  992        15.621445\n",
       "993  993        13.420916\n",
       "994  994        20.080273\n",
       "995  995        21.410704\n",
       "996  996        34.176884\n",
       "\n",
       "[997 rows x 2 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d1.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, row in testing_feature.iterrows():\n",
    "        input_tensor = torch.tensor(row.to_numpy(dtype=np.float32), dtype=torch.float32).unsqueeze(0)\n",
    "        output = model_d1(input_tensor)\n",
    "        predictions.append(output.item())\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': range(len(predictions)),\n",
    "    'tested_positive': predictions\n",
    "})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ea13e-0da6-4094-89b5-219c3281610d",
   "metadata": {},
   "source": [
    "## Train d1 Model with validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8eedfb69-bdc7-479a-9744-a7d567705b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_to_train_and_validate(df, label, val_ratio=0.2):\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(420)\n",
    "    \n",
    "    # Generate random shuffled indices\n",
    "    indices = np.random.permutation(len(df))\n",
    "    \n",
    "    # Define split size\n",
    "    val_size = int(len(df) * val_ratio)\n",
    "    train_size = len(df) - val_size\n",
    "    \n",
    "    # Split indices\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:]\n",
    "    \n",
    "    # Split the DataFrame and labels\n",
    "    X_train = df.iloc[train_indices].reset_index(drop=True)\n",
    "    y_train = label.iloc[train_indices].reset_index(drop=True)\n",
    "    \n",
    "    X_val = df.iloc[val_indices].reset_index(drop=True)\n",
    "    y_val = label.iloc[val_indices].reset_index(drop=True)\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def train_model_with_validation(model, dataloader_train, dataloader_validation, num_epochs, lr=1e-3):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = 1e9\n",
    "    nonbest_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, val_loss = 0.0, 0.0\n",
    "        model.train()\n",
    "        for batch_X, batch_y in dataloader_train:\n",
    "            preds = model(batch_X).squeeze()  # Make sure shape is (N,)\n",
    "            loss = criterion(preds, batch_y.squeeze())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch_X.size(0)\n",
    "        train_loss /= len(dataloader_train.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in dataloader_validation:\n",
    "                preds = model(batch_X).squeeze()\n",
    "                loss = criterion(preds, batch_y.squeeze())\n",
    "                val_loss += loss.item() * batch_X.size(0)\n",
    "        val_loss /= len(dataloader_validation.dataset)\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            nonbest_count = 0\n",
    "        else:\n",
    "            nonbest_count += 1\n",
    "            if nonbest_count == 600:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "aa458070-1f44-4f9d-80d1-ff4db68deabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_feature_tv_split, training_label_tv_split, validating_feature_tv_split, validating_label_tv_split = split_to_train_and_validate(training_feature, training_label, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9b4e5e49-eae9-48e1-8781-80c9657d3aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/5000, Train Loss: 35.7848, Val Loss: 32.3060\n",
      "Epoch 40/5000, Train Loss: 21.5568, Val Loss: 19.4601\n",
      "Epoch 60/5000, Train Loss: 14.3787, Val Loss: 13.4598\n",
      "Epoch 80/5000, Train Loss: 11.6518, Val Loss: 11.4428\n",
      "Epoch 100/5000, Train Loss: 10.5098, Val Loss: 10.5961\n",
      "Epoch 120/5000, Train Loss: 9.6719, Val Loss: 9.8362\n",
      "Epoch 140/5000, Train Loss: 8.8278, Val Loss: 8.9456\n",
      "Epoch 160/5000, Train Loss: 7.9455, Val Loss: 8.0144\n",
      "Epoch 180/5000, Train Loss: 7.0597, Val Loss: 7.0730\n",
      "Epoch 200/5000, Train Loss: 6.1878, Val Loss: 6.1623\n",
      "Epoch 220/5000, Train Loss: 5.3645, Val Loss: 5.3018\n",
      "Epoch 240/5000, Train Loss: 4.6120, Val Loss: 4.5075\n",
      "Epoch 260/5000, Train Loss: 3.9301, Val Loss: 3.8202\n",
      "Epoch 280/5000, Train Loss: 3.3409, Val Loss: 3.2448\n",
      "Epoch 300/5000, Train Loss: 2.8526, Val Loss: 2.7432\n",
      "Epoch 320/5000, Train Loss: 2.4469, Val Loss: 2.3538\n",
      "Epoch 340/5000, Train Loss: 2.1238, Val Loss: 2.0530\n",
      "Epoch 360/5000, Train Loss: 1.8764, Val Loss: 1.8159\n",
      "Epoch 380/5000, Train Loss: 1.7000, Val Loss: 1.6608\n",
      "Epoch 400/5000, Train Loss: 1.5592, Val Loss: 1.5324\n",
      "Epoch 420/5000, Train Loss: 1.4639, Val Loss: 1.4574\n",
      "Epoch 440/5000, Train Loss: 1.4056, Val Loss: 1.4146\n",
      "Epoch 460/5000, Train Loss: 1.3578, Val Loss: 1.3760\n",
      "Epoch 480/5000, Train Loss: 1.3233, Val Loss: 1.3556\n",
      "Epoch 500/5000, Train Loss: 1.3119, Val Loss: 1.3373\n",
      "Epoch 520/5000, Train Loss: 1.2834, Val Loss: 1.3413\n",
      "Epoch 540/5000, Train Loss: 1.2705, Val Loss: 1.3117\n",
      "Epoch 560/5000, Train Loss: 1.2499, Val Loss: 1.3165\n",
      "Epoch 580/5000, Train Loss: 1.2548, Val Loss: 1.3090\n",
      "Epoch 600/5000, Train Loss: 1.2328, Val Loss: 1.2900\n",
      "Epoch 620/5000, Train Loss: 1.2268, Val Loss: 1.2777\n",
      "Epoch 640/5000, Train Loss: 1.2154, Val Loss: 1.2752\n",
      "Epoch 660/5000, Train Loss: 1.2196, Val Loss: 1.2925\n",
      "Epoch 680/5000, Train Loss: 1.1962, Val Loss: 1.2606\n",
      "Epoch 700/5000, Train Loss: 1.1876, Val Loss: 1.2500\n",
      "Epoch 720/5000, Train Loss: 1.1773, Val Loss: 1.2448\n",
      "Epoch 740/5000, Train Loss: 1.1685, Val Loss: 1.2547\n",
      "Epoch 760/5000, Train Loss: 1.1830, Val Loss: 1.2449\n",
      "Epoch 780/5000, Train Loss: 1.1574, Val Loss: 1.2281\n",
      "Epoch 800/5000, Train Loss: 1.1601, Val Loss: 1.2237\n",
      "Epoch 820/5000, Train Loss: 1.1443, Val Loss: 1.2161\n",
      "Epoch 840/5000, Train Loss: 1.1438, Val Loss: 1.2231\n",
      "Epoch 860/5000, Train Loss: 1.1404, Val Loss: 1.2067\n",
      "Epoch 880/5000, Train Loss: 1.1470, Val Loss: 1.2041\n",
      "Epoch 900/5000, Train Loss: 1.1444, Val Loss: 1.2276\n",
      "Epoch 920/5000, Train Loss: 1.1174, Val Loss: 1.1917\n",
      "Epoch 940/5000, Train Loss: 1.1193, Val Loss: 1.2174\n",
      "Epoch 960/5000, Train Loss: 1.1234, Val Loss: 1.1975\n",
      "Epoch 980/5000, Train Loss: 1.1036, Val Loss: 1.1795\n",
      "Epoch 1000/5000, Train Loss: 1.1061, Val Loss: 1.1845\n",
      "Epoch 1020/5000, Train Loss: 1.0929, Val Loss: 1.1793\n",
      "Epoch 1040/5000, Train Loss: 1.1000, Val Loss: 1.1786\n",
      "Epoch 1060/5000, Train Loss: 1.0910, Val Loss: 1.1910\n",
      "Epoch 1080/5000, Train Loss: 1.0821, Val Loss: 1.1584\n",
      "Epoch 1100/5000, Train Loss: 1.0797, Val Loss: 1.1553\n",
      "Epoch 1120/5000, Train Loss: 1.0760, Val Loss: 1.1511\n",
      "Epoch 1140/5000, Train Loss: 1.0706, Val Loss: 1.1479\n",
      "Epoch 1160/5000, Train Loss: 1.0705, Val Loss: 1.1593\n",
      "Epoch 1180/5000, Train Loss: 1.0682, Val Loss: 1.1408\n",
      "Epoch 1200/5000, Train Loss: 1.0616, Val Loss: 1.1368\n",
      "Epoch 1220/5000, Train Loss: 1.0563, Val Loss: 1.1451\n",
      "Epoch 1240/5000, Train Loss: 1.0554, Val Loss: 1.1344\n",
      "Epoch 1260/5000, Train Loss: 1.0509, Val Loss: 1.1273\n",
      "Epoch 1280/5000, Train Loss: 1.0470, Val Loss: 1.1234\n",
      "Epoch 1300/5000, Train Loss: 1.0437, Val Loss: 1.1297\n",
      "Epoch 1320/5000, Train Loss: 1.0383, Val Loss: 1.1183\n",
      "Epoch 1340/5000, Train Loss: 1.0513, Val Loss: 1.1148\n",
      "Epoch 1360/5000, Train Loss: 1.0404, Val Loss: 1.1388\n",
      "Epoch 1380/5000, Train Loss: 1.0289, Val Loss: 1.1081\n",
      "Epoch 1400/5000, Train Loss: 1.0286, Val Loss: 1.1149\n",
      "Epoch 1420/5000, Train Loss: 1.0291, Val Loss: 1.1021\n",
      "Epoch 1440/5000, Train Loss: 1.0268, Val Loss: 1.1052\n",
      "Epoch 1460/5000, Train Loss: 1.0167, Val Loss: 1.0972\n",
      "Epoch 1480/5000, Train Loss: 1.0155, Val Loss: 1.1046\n",
      "Epoch 1500/5000, Train Loss: 1.0148, Val Loss: 1.0960\n",
      "Epoch 1520/5000, Train Loss: 1.0113, Val Loss: 1.0919\n",
      "Epoch 1540/5000, Train Loss: 1.0071, Val Loss: 1.0893\n",
      "Epoch 1560/5000, Train Loss: 1.0009, Val Loss: 1.0855\n",
      "Epoch 1580/5000, Train Loss: 1.0212, Val Loss: 1.1130\n",
      "Epoch 1600/5000, Train Loss: 1.0052, Val Loss: 1.0824\n",
      "Epoch 1620/5000, Train Loss: 1.0097, Val Loss: 1.0970\n",
      "Epoch 1640/5000, Train Loss: 0.9964, Val Loss: 1.0758\n",
      "Epoch 1660/5000, Train Loss: 0.9968, Val Loss: 1.0815\n",
      "Epoch 1680/5000, Train Loss: 0.9940, Val Loss: 1.0702\n",
      "Epoch 1700/5000, Train Loss: 0.9890, Val Loss: 1.0678\n",
      "Epoch 1720/5000, Train Loss: 0.9912, Val Loss: 1.0699\n",
      "Epoch 1740/5000, Train Loss: 0.9830, Val Loss: 1.0642\n",
      "Epoch 1760/5000, Train Loss: 0.9812, Val Loss: 1.0730\n",
      "Epoch 1780/5000, Train Loss: 0.9905, Val Loss: 1.0649\n",
      "Epoch 1800/5000, Train Loss: 0.9773, Val Loss: 1.0578\n",
      "Epoch 1820/5000, Train Loss: 0.9761, Val Loss: 1.0583\n",
      "Epoch 1840/5000, Train Loss: 0.9736, Val Loss: 1.0538\n",
      "Epoch 1860/5000, Train Loss: 0.9713, Val Loss: 1.0517\n",
      "Epoch 1880/5000, Train Loss: 0.9709, Val Loss: 1.0511\n",
      "Epoch 1900/5000, Train Loss: 0.9688, Val Loss: 1.0669\n",
      "Epoch 1920/5000, Train Loss: 0.9656, Val Loss: 1.0473\n",
      "Epoch 1940/5000, Train Loss: 0.9678, Val Loss: 1.0448\n",
      "Epoch 1960/5000, Train Loss: 0.9679, Val Loss: 1.0432\n",
      "Epoch 1980/5000, Train Loss: 0.9627, Val Loss: 1.0446\n",
      "Epoch 2000/5000, Train Loss: 0.9593, Val Loss: 1.0401\n",
      "Epoch 2020/5000, Train Loss: 0.9575, Val Loss: 1.0587\n",
      "Epoch 2040/5000, Train Loss: 0.9679, Val Loss: 1.0405\n",
      "Epoch 2060/5000, Train Loss: 0.9557, Val Loss: 1.0382\n",
      "Epoch 2080/5000, Train Loss: 0.9602, Val Loss: 1.0383\n",
      "Epoch 2100/5000, Train Loss: 0.9520, Val Loss: 1.0842\n",
      "Epoch 2120/5000, Train Loss: 0.9524, Val Loss: 1.0311\n",
      "Epoch 2140/5000, Train Loss: 0.9511, Val Loss: 1.0438\n",
      "Epoch 2160/5000, Train Loss: 0.9500, Val Loss: 1.0284\n",
      "Epoch 2180/5000, Train Loss: 0.9469, Val Loss: 1.0265\n",
      "Epoch 2200/5000, Train Loss: 0.9485, Val Loss: 1.0278\n",
      "Epoch 2220/5000, Train Loss: 0.9420, Val Loss: 1.0262\n",
      "Epoch 2240/5000, Train Loss: 0.9415, Val Loss: 1.0253\n",
      "Epoch 2260/5000, Train Loss: 0.9537, Val Loss: 1.0221\n",
      "Epoch 2280/5000, Train Loss: 0.9380, Val Loss: 1.0198\n",
      "Epoch 2300/5000, Train Loss: 0.9397, Val Loss: 1.0201\n",
      "Epoch 2320/5000, Train Loss: 0.9359, Val Loss: 1.0173\n",
      "Epoch 2340/5000, Train Loss: 0.9431, Val Loss: 1.0198\n",
      "Epoch 2360/5000, Train Loss: 0.9359, Val Loss: 1.0160\n",
      "Epoch 2380/5000, Train Loss: 0.9353, Val Loss: 1.0227\n",
      "Epoch 2400/5000, Train Loss: 0.9324, Val Loss: 1.0191\n",
      "Epoch 2420/5000, Train Loss: 0.9395, Val Loss: 1.0263\n",
      "Epoch 2440/5000, Train Loss: 0.9338, Val Loss: 1.0340\n",
      "Epoch 2460/5000, Train Loss: 0.9285, Val Loss: 1.0095\n",
      "Epoch 2480/5000, Train Loss: 0.9288, Val Loss: 1.0128\n",
      "Epoch 2500/5000, Train Loss: 0.9244, Val Loss: 1.0072\n",
      "Epoch 2520/5000, Train Loss: 0.9271, Val Loss: 1.0061\n",
      "Epoch 2540/5000, Train Loss: 0.9260, Val Loss: 1.0141\n",
      "Epoch 2560/5000, Train Loss: 0.9214, Val Loss: 1.0040\n",
      "Epoch 2580/5000, Train Loss: 0.9221, Val Loss: 1.0034\n",
      "Epoch 2600/5000, Train Loss: 0.9222, Val Loss: 1.0019\n",
      "Epoch 2620/5000, Train Loss: 0.9334, Val Loss: 1.0142\n",
      "Epoch 2640/5000, Train Loss: 0.9199, Val Loss: 1.0053\n",
      "Epoch 2660/5000, Train Loss: 0.9196, Val Loss: 0.9992\n",
      "Epoch 2680/5000, Train Loss: 0.9194, Val Loss: 1.0012\n",
      "Epoch 2700/5000, Train Loss: 0.9309, Val Loss: 1.0006\n",
      "Epoch 2720/5000, Train Loss: 0.9282, Val Loss: 1.0129\n",
      "Epoch 2740/5000, Train Loss: 0.9155, Val Loss: 0.9959\n",
      "Epoch 2760/5000, Train Loss: 0.9108, Val Loss: 1.0047\n",
      "Epoch 2780/5000, Train Loss: 0.9286, Val Loss: 1.0127\n",
      "Epoch 2800/5000, Train Loss: 0.9095, Val Loss: 0.9928\n",
      "Epoch 2820/5000, Train Loss: 0.9122, Val Loss: 0.9979\n",
      "Epoch 2840/5000, Train Loss: 0.9147, Val Loss: 0.9959\n",
      "Epoch 2860/5000, Train Loss: 0.9096, Val Loss: 0.9922\n",
      "Epoch 2880/5000, Train Loss: 0.9136, Val Loss: 0.9981\n",
      "Epoch 2900/5000, Train Loss: 0.9064, Val Loss: 1.0068\n",
      "Epoch 2920/5000, Train Loss: 0.9041, Val Loss: 0.9894\n",
      "Epoch 2940/5000, Train Loss: 0.9088, Val Loss: 1.0065\n",
      "Epoch 2960/5000, Train Loss: 0.9037, Val Loss: 0.9892\n",
      "Epoch 2980/5000, Train Loss: 0.9155, Val Loss: 1.0074\n",
      "Epoch 3000/5000, Train Loss: 0.9040, Val Loss: 0.9856\n",
      "Epoch 3020/5000, Train Loss: 0.9016, Val Loss: 0.9887\n",
      "Epoch 3040/5000, Train Loss: 0.9059, Val Loss: 0.9883\n",
      "Epoch 3060/5000, Train Loss: 0.9021, Val Loss: 0.9847\n",
      "Epoch 3080/5000, Train Loss: 0.8991, Val Loss: 0.9910\n",
      "Epoch 3100/5000, Train Loss: 0.9018, Val Loss: 0.9819\n",
      "Epoch 3120/5000, Train Loss: 0.8971, Val Loss: 0.9818\n",
      "Epoch 3140/5000, Train Loss: 0.9005, Val Loss: 0.9805\n",
      "Epoch 3160/5000, Train Loss: 0.8956, Val Loss: 0.9809\n",
      "Epoch 3180/5000, Train Loss: 0.8959, Val Loss: 0.9836\n",
      "Epoch 3200/5000, Train Loss: 0.8945, Val Loss: 0.9804\n",
      "Epoch 3220/5000, Train Loss: 0.8959, Val Loss: 0.9875\n",
      "Epoch 3240/5000, Train Loss: 0.8962, Val Loss: 0.9779\n",
      "Epoch 3260/5000, Train Loss: 0.8969, Val Loss: 0.9809\n",
      "Epoch 3280/5000, Train Loss: 0.8980, Val Loss: 0.9764\n",
      "Epoch 3300/5000, Train Loss: 0.8974, Val Loss: 0.9815\n",
      "Epoch 3320/5000, Train Loss: 0.8945, Val Loss: 0.9770\n",
      "Epoch 3340/5000, Train Loss: 0.8915, Val Loss: 0.9754\n",
      "Epoch 3360/5000, Train Loss: 0.8950, Val Loss: 0.9741\n",
      "Epoch 3380/5000, Train Loss: 0.9019, Val Loss: 0.9744\n",
      "Epoch 3400/5000, Train Loss: 0.8933, Val Loss: 0.9732\n",
      "Epoch 3420/5000, Train Loss: 0.8920, Val Loss: 0.9750\n",
      "Epoch 3440/5000, Train Loss: 0.8941, Val Loss: 0.9733\n",
      "Epoch 3460/5000, Train Loss: 0.9066, Val Loss: 1.0123\n"
     ]
    }
   ],
   "source": [
    "dataset_train = CovidDataset(training_feature_tv_split, training_label_tv_split)\n",
    "dataset_val = CovidDataset(validating_feature_tv_split, validating_label_tv_split)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=128, shuffle=True)\n",
    "\n",
    "model_d1_tv_split = RegressionModel_d1(input_dim=53)\n",
    "train_model_with_validation(model_d1_tv_split, dataloader_train, dataloader_val, 5000, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5d17e228-cc5c-491f-889b-8d21032fbfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tested_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>32.421753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.041031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>25.149904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>26.722281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>40.303959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>992</td>\n",
       "      <td>16.168726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>993</td>\n",
       "      <td>13.318082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>994</td>\n",
       "      <td>19.786514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>21.341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>34.293533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>997 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  tested_positive\n",
       "0      0        32.421753\n",
       "1      1         6.041031\n",
       "2      2        25.149904\n",
       "3      3        26.722281\n",
       "4      4        40.303959\n",
       "..   ...              ...\n",
       "992  992        16.168726\n",
       "993  993        13.318082\n",
       "994  994        19.786514\n",
       "995  995        21.341900\n",
       "996  996        34.293533\n",
       "\n",
       "[997 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d1_tv_split.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, row in testing_feature.iterrows():\n",
    "        input_tensor = torch.tensor(row.to_numpy(dtype=np.float32), dtype=torch.float32).unsqueeze(0)\n",
    "        output = model_d1_tv_split(input_tensor)\n",
    "        predictions.append(output.item())\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': range(len(predictions)),\n",
    "    'tested_positive': predictions\n",
    "})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7f2467-ecc5-48d3-94bd-87d01adb8648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc88e585-2358-4523-8983-50d165e77d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f44f7-d7e8-43a9-a519-7b833d5affaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "120fa280-1140-4880-8c55-3f1d091c8ec7",
   "metadata": {},
   "source": [
    "## d1 + tv split + Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "910170a7-0612-41c3-add7-7a23fb3da4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_min_max(df):\n",
    "    min_vals = df.min()\n",
    "    max_vals = df.max()\n",
    "    return min_vals, max_vals\n",
    "\n",
    "def apply_min_max_normalization(df, min_vals, max_vals):\n",
    "    denominator = max_vals - min_vals\n",
    "    denominator = denominator.apply(lambda x : x if x > 1e-3 else 1)\n",
    "    df_normalized = (df - min_vals) / denominator\n",
    "    return df_normalized\n",
    "\n",
    "# 1. Compute min/max from training set\n",
    "min_vals, max_vals = compute_min_max(training_feature)\n",
    "\n",
    "# 2. Normalize training set\n",
    "training_feature_normalized = apply_min_max_normalization(training_feature, min_vals, max_vals)\n",
    "\n",
    "# 3. Normalize testing set (using training's min/max)\n",
    "testing_feature_normalized = apply_min_max_normalization(testing_feature, min_vals, max_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bdd2c5d2-93ec-442f-b26f-f663fe9e6799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cli</th>\n",
       "      <th>ili</th>\n",
       "      <th>wnohh_cmnty_cli</th>\n",
       "      <th>wbelief_masking_effective</th>\n",
       "      <th>wbelief_distancing_effective</th>\n",
       "      <th>wcovid_vaccinated_friends</th>\n",
       "      <th>wlarge_event_indoors</th>\n",
       "      <th>wothers_masked_public</th>\n",
       "      <th>wothers_distanced_public</th>\n",
       "      <th>wshop_indoors</th>\n",
       "      <th>...</th>\n",
       "      <th>wothers_masked_public.2</th>\n",
       "      <th>wothers_distanced_public.2</th>\n",
       "      <th>wshop_indoors.2</th>\n",
       "      <th>wrestaurant_indoors.2</th>\n",
       "      <th>wworried_catch_covid.2</th>\n",
       "      <th>hh_cmnty_cli.2</th>\n",
       "      <th>nohh_cmnty_cli.2</th>\n",
       "      <th>wearing_mask_7d.2</th>\n",
       "      <th>public_transit.2</th>\n",
       "      <th>worried_finances.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.142552</td>\n",
       "      <td>0.138633</td>\n",
       "      <td>0.268012</td>\n",
       "      <td>0.435716</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>0.190381</td>\n",
       "      <td>0.490897</td>\n",
       "      <td>0.180109</td>\n",
       "      <td>0.511400</td>\n",
       "      <td>0.804858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175177</td>\n",
       "      <td>0.455554</td>\n",
       "      <td>0.773497</td>\n",
       "      <td>0.599623</td>\n",
       "      <td>0.489868</td>\n",
       "      <td>0.258572</td>\n",
       "      <td>0.245222</td>\n",
       "      <td>0.559910</td>\n",
       "      <td>0.065018</td>\n",
       "      <td>0.621288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.134187</td>\n",
       "      <td>0.131449</td>\n",
       "      <td>0.248755</td>\n",
       "      <td>0.424849</td>\n",
       "      <td>0.403573</td>\n",
       "      <td>0.157819</td>\n",
       "      <td>0.507229</td>\n",
       "      <td>0.183230</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.759854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185694</td>\n",
       "      <td>0.467024</td>\n",
       "      <td>0.763836</td>\n",
       "      <td>0.575090</td>\n",
       "      <td>0.476887</td>\n",
       "      <td>0.254587</td>\n",
       "      <td>0.237533</td>\n",
       "      <td>0.557467</td>\n",
       "      <td>0.048270</td>\n",
       "      <td>0.727513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.113327</td>\n",
       "      <td>0.118104</td>\n",
       "      <td>0.238259</td>\n",
       "      <td>0.421506</td>\n",
       "      <td>0.440005</td>\n",
       "      <td>0.160289</td>\n",
       "      <td>0.526149</td>\n",
       "      <td>0.175177</td>\n",
       "      <td>0.455554</td>\n",
       "      <td>0.773497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187916</td>\n",
       "      <td>0.438933</td>\n",
       "      <td>0.756577</td>\n",
       "      <td>0.581672</td>\n",
       "      <td>0.483233</td>\n",
       "      <td>0.255012</td>\n",
       "      <td>0.235604</td>\n",
       "      <td>0.553876</td>\n",
       "      <td>0.055202</td>\n",
       "      <td>0.643450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.127636</td>\n",
       "      <td>0.135427</td>\n",
       "      <td>0.228387</td>\n",
       "      <td>0.410904</td>\n",
       "      <td>0.447816</td>\n",
       "      <td>0.194967</td>\n",
       "      <td>0.497776</td>\n",
       "      <td>0.185694</td>\n",
       "      <td>0.467024</td>\n",
       "      <td>0.763836</td>\n",
       "      <td>...</td>\n",
       "      <td>0.173458</td>\n",
       "      <td>0.426924</td>\n",
       "      <td>0.787250</td>\n",
       "      <td>0.588367</td>\n",
       "      <td>0.428928</td>\n",
       "      <td>0.252315</td>\n",
       "      <td>0.234358</td>\n",
       "      <td>0.537493</td>\n",
       "      <td>0.058132</td>\n",
       "      <td>0.605488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.123918</td>\n",
       "      <td>0.130442</td>\n",
       "      <td>0.225106</td>\n",
       "      <td>0.401872</td>\n",
       "      <td>0.448888</td>\n",
       "      <td>0.206202</td>\n",
       "      <td>0.496443</td>\n",
       "      <td>0.187916</td>\n",
       "      <td>0.438933</td>\n",
       "      <td>0.756577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.180351</td>\n",
       "      <td>0.393745</td>\n",
       "      <td>0.797968</td>\n",
       "      <td>0.607325</td>\n",
       "      <td>0.415909</td>\n",
       "      <td>0.233930</td>\n",
       "      <td>0.216142</td>\n",
       "      <td>0.528195</td>\n",
       "      <td>0.071320</td>\n",
       "      <td>0.654329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3004</th>\n",
       "      <td>0.089710</td>\n",
       "      <td>0.090941</td>\n",
       "      <td>0.123264</td>\n",
       "      <td>0.238313</td>\n",
       "      <td>0.228164</td>\n",
       "      <td>0.440685</td>\n",
       "      <td>0.857365</td>\n",
       "      <td>0.031021</td>\n",
       "      <td>0.044948</td>\n",
       "      <td>0.664518</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040846</td>\n",
       "      <td>0.089452</td>\n",
       "      <td>0.684128</td>\n",
       "      <td>0.829434</td>\n",
       "      <td>0.185890</td>\n",
       "      <td>0.114763</td>\n",
       "      <td>0.114323</td>\n",
       "      <td>0.017505</td>\n",
       "      <td>0.310325</td>\n",
       "      <td>0.152342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3005</th>\n",
       "      <td>0.086237</td>\n",
       "      <td>0.087498</td>\n",
       "      <td>0.122116</td>\n",
       "      <td>0.245540</td>\n",
       "      <td>0.232336</td>\n",
       "      <td>0.458175</td>\n",
       "      <td>0.872557</td>\n",
       "      <td>0.039056</td>\n",
       "      <td>0.065022</td>\n",
       "      <td>0.683558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046152</td>\n",
       "      <td>0.118264</td>\n",
       "      <td>0.662891</td>\n",
       "      <td>0.820922</td>\n",
       "      <td>0.159352</td>\n",
       "      <td>0.121470</td>\n",
       "      <td>0.123244</td>\n",
       "      <td>0.027305</td>\n",
       "      <td>0.321377</td>\n",
       "      <td>0.192199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>0.075734</td>\n",
       "      <td>0.077191</td>\n",
       "      <td>0.121955</td>\n",
       "      <td>0.285821</td>\n",
       "      <td>0.271589</td>\n",
       "      <td>0.475161</td>\n",
       "      <td>0.839085</td>\n",
       "      <td>0.040846</td>\n",
       "      <td>0.089452</td>\n",
       "      <td>0.684128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045364</td>\n",
       "      <td>0.155548</td>\n",
       "      <td>0.671843</td>\n",
       "      <td>0.831456</td>\n",
       "      <td>0.146827</td>\n",
       "      <td>0.117165</td>\n",
       "      <td>0.115297</td>\n",
       "      <td>0.018211</td>\n",
       "      <td>0.317107</td>\n",
       "      <td>0.207997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3007</th>\n",
       "      <td>0.099212</td>\n",
       "      <td>0.100072</td>\n",
       "      <td>0.135887</td>\n",
       "      <td>0.318358</td>\n",
       "      <td>0.249971</td>\n",
       "      <td>0.489927</td>\n",
       "      <td>0.840185</td>\n",
       "      <td>0.046152</td>\n",
       "      <td>0.118264</td>\n",
       "      <td>0.662891</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034349</td>\n",
       "      <td>0.101270</td>\n",
       "      <td>0.736754</td>\n",
       "      <td>0.877855</td>\n",
       "      <td>0.129317</td>\n",
       "      <td>0.125922</td>\n",
       "      <td>0.118604</td>\n",
       "      <td>0.006107</td>\n",
       "      <td>0.297255</td>\n",
       "      <td>0.253213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3008</th>\n",
       "      <td>0.105950</td>\n",
       "      <td>0.100018</td>\n",
       "      <td>0.132781</td>\n",
       "      <td>0.312845</td>\n",
       "      <td>0.227192</td>\n",
       "      <td>0.472539</td>\n",
       "      <td>0.861974</td>\n",
       "      <td>0.045364</td>\n",
       "      <td>0.155548</td>\n",
       "      <td>0.671843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028568</td>\n",
       "      <td>0.101640</td>\n",
       "      <td>0.742216</td>\n",
       "      <td>0.873878</td>\n",
       "      <td>0.111507</td>\n",
       "      <td>0.121911</td>\n",
       "      <td>0.112873</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.318664</td>\n",
       "      <td>0.227708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3009 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           cli       ili  wnohh_cmnty_cli  wbelief_masking_effective  \\\n",
       "0     0.142552  0.138633         0.268012                   0.435716   \n",
       "1     0.134187  0.131449         0.248755                   0.424849   \n",
       "2     0.113327  0.118104         0.238259                   0.421506   \n",
       "3     0.127636  0.135427         0.228387                   0.410904   \n",
       "4     0.123918  0.130442         0.225106                   0.401872   \n",
       "...        ...       ...              ...                        ...   \n",
       "3004  0.089710  0.090941         0.123264                   0.238313   \n",
       "3005  0.086237  0.087498         0.122116                   0.245540   \n",
       "3006  0.075734  0.077191         0.121955                   0.285821   \n",
       "3007  0.099212  0.100072         0.135887                   0.318358   \n",
       "3008  0.105950  0.100018         0.132781                   0.312845   \n",
       "\n",
       "      wbelief_distancing_effective  wcovid_vaccinated_friends  \\\n",
       "0                         0.435100                   0.190381   \n",
       "1                         0.403573                   0.157819   \n",
       "2                         0.440005                   0.160289   \n",
       "3                         0.447816                   0.194967   \n",
       "4                         0.448888                   0.206202   \n",
       "...                            ...                        ...   \n",
       "3004                      0.228164                   0.440685   \n",
       "3005                      0.232336                   0.458175   \n",
       "3006                      0.271589                   0.475161   \n",
       "3007                      0.249971                   0.489927   \n",
       "3008                      0.227192                   0.472539   \n",
       "\n",
       "      wlarge_event_indoors  wothers_masked_public  wothers_distanced_public  \\\n",
       "0                 0.490897               0.180109                  0.511400   \n",
       "1                 0.507229               0.183230                  0.444444   \n",
       "2                 0.526149               0.175177                  0.455554   \n",
       "3                 0.497776               0.185694                  0.467024   \n",
       "4                 0.496443               0.187916                  0.438933   \n",
       "...                    ...                    ...                       ...   \n",
       "3004              0.857365               0.031021                  0.044948   \n",
       "3005              0.872557               0.039056                  0.065022   \n",
       "3006              0.839085               0.040846                  0.089452   \n",
       "3007              0.840185               0.046152                  0.118264   \n",
       "3008              0.861974               0.045364                  0.155548   \n",
       "\n",
       "      wshop_indoors  ...  wothers_masked_public.2  wothers_distanced_public.2  \\\n",
       "0          0.804858  ...                 0.175177                    0.455554   \n",
       "1          0.759854  ...                 0.185694                    0.467024   \n",
       "2          0.773497  ...                 0.187916                    0.438933   \n",
       "3          0.763836  ...                 0.173458                    0.426924   \n",
       "4          0.756577  ...                 0.180351                    0.393745   \n",
       "...             ...  ...                      ...                         ...   \n",
       "3004       0.664518  ...                 0.040846                    0.089452   \n",
       "3005       0.683558  ...                 0.046152                    0.118264   \n",
       "3006       0.684128  ...                 0.045364                    0.155548   \n",
       "3007       0.662891  ...                 0.034349                    0.101270   \n",
       "3008       0.671843  ...                 0.028568                    0.101640   \n",
       "\n",
       "      wshop_indoors.2  wrestaurant_indoors.2  wworried_catch_covid.2  \\\n",
       "0            0.773497               0.599623                0.489868   \n",
       "1            0.763836               0.575090                0.476887   \n",
       "2            0.756577               0.581672                0.483233   \n",
       "3            0.787250               0.588367                0.428928   \n",
       "4            0.797968               0.607325                0.415909   \n",
       "...               ...                    ...                     ...   \n",
       "3004         0.684128               0.829434                0.185890   \n",
       "3005         0.662891               0.820922                0.159352   \n",
       "3006         0.671843               0.831456                0.146827   \n",
       "3007         0.736754               0.877855                0.129317   \n",
       "3008         0.742216               0.873878                0.111507   \n",
       "\n",
       "      hh_cmnty_cli.2  nohh_cmnty_cli.2  wearing_mask_7d.2  public_transit.2  \\\n",
       "0           0.258572          0.245222           0.559910          0.065018   \n",
       "1           0.254587          0.237533           0.557467          0.048270   \n",
       "2           0.255012          0.235604           0.553876          0.055202   \n",
       "3           0.252315          0.234358           0.537493          0.058132   \n",
       "4           0.233930          0.216142           0.528195          0.071320   \n",
       "...              ...               ...                ...               ...   \n",
       "3004        0.114763          0.114323           0.017505          0.310325   \n",
       "3005        0.121470          0.123244           0.027305          0.321377   \n",
       "3006        0.117165          0.115297           0.018211          0.317107   \n",
       "3007        0.125922          0.118604           0.006107          0.297255   \n",
       "3008        0.121911          0.112873           0.000000          0.318664   \n",
       "\n",
       "      worried_finances.2  \n",
       "0               0.621288  \n",
       "1               0.727513  \n",
       "2               0.643450  \n",
       "3               0.605488  \n",
       "4               0.654329  \n",
       "...                  ...  \n",
       "3004            0.152342  \n",
       "3005            0.192199  \n",
       "3006            0.207997  \n",
       "3007            0.253213  \n",
       "3008            0.227708  \n",
       "\n",
       "[3009 rows x 53 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_feature_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "a51e9fa3-7d1a-4eae-a775-603ee9bddbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/10000, Train Loss: 104.3003, Val Loss: 108.1841\n",
      "Epoch 40/10000, Train Loss: 64.4833, Val Loss: 63.7665\n",
      "Epoch 60/10000, Train Loss: 46.0896, Val Loss: 45.0718\n",
      "Epoch 80/10000, Train Loss: 32.1074, Val Loss: 31.4564\n",
      "Epoch 100/10000, Train Loss: 22.7232, Val Loss: 22.4192\n",
      "Epoch 120/10000, Train Loss: 17.1422, Val Loss: 17.1412\n",
      "Epoch 140/10000, Train Loss: 14.1594, Val Loss: 14.3621\n",
      "Epoch 160/10000, Train Loss: 12.6276, Val Loss: 12.9447\n",
      "Epoch 180/10000, Train Loss: 11.7412, Val Loss: 12.0953\n",
      "Epoch 200/10000, Train Loss: 11.0606, Val Loss: 11.3991\n",
      "Epoch 220/10000, Train Loss: 10.4150, Val Loss: 10.7147\n",
      "Epoch 240/10000, Train Loss: 9.7604, Val Loss: 9.9926\n",
      "Epoch 260/10000, Train Loss: 9.1017, Val Loss: 9.2755\n",
      "Epoch 280/10000, Train Loss: 8.4449, Val Loss: 8.5679\n",
      "Epoch 300/10000, Train Loss: 7.8154, Val Loss: 7.8758\n",
      "Epoch 320/10000, Train Loss: 7.1899, Val Loss: 7.2086\n",
      "Epoch 340/10000, Train Loss: 6.6047, Val Loss: 6.5824\n",
      "Epoch 360/10000, Train Loss: 6.0568, Val Loss: 6.0144\n",
      "Epoch 380/10000, Train Loss: 5.5498, Val Loss: 5.4926\n",
      "Epoch 400/10000, Train Loss: 5.0869, Val Loss: 5.0265\n",
      "Epoch 420/10000, Train Loss: 4.6656, Val Loss: 4.5962\n",
      "Epoch 440/10000, Train Loss: 4.2749, Val Loss: 4.2184\n",
      "Epoch 460/10000, Train Loss: 3.9267, Val Loss: 3.8844\n",
      "Epoch 480/10000, Train Loss: 3.6160, Val Loss: 3.5818\n",
      "Epoch 500/10000, Train Loss: 3.3350, Val Loss: 3.3201\n",
      "Epoch 520/10000, Train Loss: 3.0860, Val Loss: 3.0864\n",
      "Epoch 540/10000, Train Loss: 2.8671, Val Loss: 2.8788\n",
      "Epoch 560/10000, Train Loss: 2.6686, Val Loss: 2.6978\n",
      "Epoch 580/10000, Train Loss: 2.4918, Val Loss: 2.5335\n",
      "Epoch 600/10000, Train Loss: 2.3329, Val Loss: 2.3878\n",
      "Epoch 620/10000, Train Loss: 2.1934, Val Loss: 2.2562\n",
      "Epoch 640/10000, Train Loss: 2.0698, Val Loss: 2.1387\n",
      "Epoch 660/10000, Train Loss: 1.9578, Val Loss: 2.0368\n",
      "Epoch 680/10000, Train Loss: 1.8601, Val Loss: 1.9476\n",
      "Epoch 700/10000, Train Loss: 1.7732, Val Loss: 1.8620\n",
      "Epoch 720/10000, Train Loss: 1.6941, Val Loss: 1.7850\n",
      "Epoch 740/10000, Train Loss: 1.6275, Val Loss: 1.7222\n",
      "Epoch 760/10000, Train Loss: 1.5651, Val Loss: 1.6593\n",
      "Epoch 780/10000, Train Loss: 1.5115, Val Loss: 1.6075\n",
      "Epoch 800/10000, Train Loss: 1.4640, Val Loss: 1.5655\n",
      "Epoch 820/10000, Train Loss: 1.4218, Val Loss: 1.5214\n",
      "Epoch 840/10000, Train Loss: 1.3926, Val Loss: 1.4940\n",
      "Epoch 860/10000, Train Loss: 1.3523, Val Loss: 1.4528\n",
      "Epoch 880/10000, Train Loss: 1.3229, Val Loss: 1.4240\n",
      "Epoch 900/10000, Train Loss: 1.2986, Val Loss: 1.4064\n",
      "Epoch 920/10000, Train Loss: 1.2724, Val Loss: 1.3793\n",
      "Epoch 940/10000, Train Loss: 1.2546, Val Loss: 1.3672\n",
      "Epoch 960/10000, Train Loss: 1.2340, Val Loss: 1.3385\n",
      "Epoch 980/10000, Train Loss: 1.2172, Val Loss: 1.3244\n",
      "Epoch 1000/10000, Train Loss: 1.2031, Val Loss: 1.3061\n",
      "Epoch 1020/10000, Train Loss: 1.1881, Val Loss: 1.2949\n",
      "Epoch 1040/10000, Train Loss: 1.1770, Val Loss: 1.2831\n",
      "Epoch 1060/10000, Train Loss: 1.1634, Val Loss: 1.2676\n",
      "Epoch 1080/10000, Train Loss: 1.1526, Val Loss: 1.2563\n",
      "Epoch 1100/10000, Train Loss: 1.1452, Val Loss: 1.2537\n",
      "Epoch 1120/10000, Train Loss: 1.1359, Val Loss: 1.2371\n",
      "Epoch 1140/10000, Train Loss: 1.1279, Val Loss: 1.2276\n",
      "Epoch 1160/10000, Train Loss: 1.1179, Val Loss: 1.2213\n",
      "Epoch 1180/10000, Train Loss: 1.1087, Val Loss: 1.2113\n",
      "Epoch 1200/10000, Train Loss: 1.1034, Val Loss: 1.2048\n",
      "Epoch 1220/10000, Train Loss: 1.0971, Val Loss: 1.2068\n",
      "Epoch 1240/10000, Train Loss: 1.0898, Val Loss: 1.1913\n",
      "Epoch 1260/10000, Train Loss: 1.0821, Val Loss: 1.1907\n",
      "Epoch 1280/10000, Train Loss: 1.0807, Val Loss: 1.1894\n",
      "Epoch 1300/10000, Train Loss: 1.0703, Val Loss: 1.1756\n",
      "Epoch 1320/10000, Train Loss: 1.0667, Val Loss: 1.1700\n",
      "Epoch 1340/10000, Train Loss: 1.0615, Val Loss: 1.1643\n",
      "Epoch 1360/10000, Train Loss: 1.0585, Val Loss: 1.1598\n",
      "Epoch 1380/10000, Train Loss: 1.0518, Val Loss: 1.1560\n",
      "Epoch 1400/10000, Train Loss: 1.0472, Val Loss: 1.1544\n",
      "Epoch 1420/10000, Train Loss: 1.0438, Val Loss: 1.1531\n",
      "Epoch 1440/10000, Train Loss: 1.0418, Val Loss: 1.1472\n",
      "Epoch 1460/10000, Train Loss: 1.0370, Val Loss: 1.1404\n",
      "Epoch 1480/10000, Train Loss: 1.0340, Val Loss: 1.1394\n",
      "Epoch 1500/10000, Train Loss: 1.0296, Val Loss: 1.1405\n",
      "Epoch 1520/10000, Train Loss: 1.0294, Val Loss: 1.1319\n",
      "Epoch 1540/10000, Train Loss: 1.0251, Val Loss: 1.1353\n",
      "Epoch 1560/10000, Train Loss: 1.0197, Val Loss: 1.1258\n",
      "Epoch 1580/10000, Train Loss: 1.0182, Val Loss: 1.1232\n",
      "Epoch 1600/10000, Train Loss: 1.0188, Val Loss: 1.1191\n",
      "Epoch 1620/10000, Train Loss: 1.0126, Val Loss: 1.1173\n",
      "Epoch 1640/10000, Train Loss: 1.0100, Val Loss: 1.1166\n",
      "Epoch 1660/10000, Train Loss: 1.0068, Val Loss: 1.1145\n",
      "Epoch 1680/10000, Train Loss: 1.0049, Val Loss: 1.1107\n",
      "Epoch 1700/10000, Train Loss: 1.0033, Val Loss: 1.1098\n",
      "Epoch 1720/10000, Train Loss: 1.0000, Val Loss: 1.1052\n",
      "Epoch 1740/10000, Train Loss: 0.9968, Val Loss: 1.1075\n",
      "Epoch 1760/10000, Train Loss: 0.9959, Val Loss: 1.1016\n",
      "Epoch 1780/10000, Train Loss: 0.9937, Val Loss: 1.0999\n",
      "Epoch 1800/10000, Train Loss: 0.9910, Val Loss: 1.1011\n",
      "Epoch 1820/10000, Train Loss: 0.9894, Val Loss: 1.0955\n",
      "Epoch 1840/10000, Train Loss: 0.9893, Val Loss: 1.1000\n",
      "Epoch 1860/10000, Train Loss: 0.9870, Val Loss: 1.0928\n",
      "Epoch 1880/10000, Train Loss: 0.9841, Val Loss: 1.0932\n",
      "Epoch 1900/10000, Train Loss: 0.9820, Val Loss: 1.0918\n",
      "Epoch 1920/10000, Train Loss: 0.9802, Val Loss: 1.0853\n",
      "Epoch 1940/10000, Train Loss: 0.9794, Val Loss: 1.0888\n",
      "Epoch 1960/10000, Train Loss: 0.9765, Val Loss: 1.0837\n",
      "Epoch 1980/10000, Train Loss: 0.9756, Val Loss: 1.0800\n",
      "Epoch 2000/10000, Train Loss: 0.9739, Val Loss: 1.0789\n",
      "Epoch 2020/10000, Train Loss: 0.9725, Val Loss: 1.0779\n",
      "Epoch 2040/10000, Train Loss: 0.9721, Val Loss: 1.0786\n",
      "Epoch 2060/10000, Train Loss: 0.9687, Val Loss: 1.0786\n",
      "Epoch 2080/10000, Train Loss: 0.9708, Val Loss: 1.0734\n",
      "Epoch 2100/10000, Train Loss: 0.9663, Val Loss: 1.0753\n",
      "Epoch 2120/10000, Train Loss: 0.9656, Val Loss: 1.0712\n",
      "Epoch 2140/10000, Train Loss: 0.9641, Val Loss: 1.0731\n",
      "Epoch 2160/10000, Train Loss: 0.9616, Val Loss: 1.0694\n",
      "Epoch 2180/10000, Train Loss: 0.9596, Val Loss: 1.0673\n",
      "Epoch 2200/10000, Train Loss: 0.9585, Val Loss: 1.0642\n",
      "Epoch 2220/10000, Train Loss: 0.9577, Val Loss: 1.0633\n",
      "Epoch 2240/10000, Train Loss: 0.9573, Val Loss: 1.0652\n",
      "Epoch 2260/10000, Train Loss: 0.9556, Val Loss: 1.0607\n",
      "Epoch 2280/10000, Train Loss: 0.9532, Val Loss: 1.0596\n",
      "Epoch 2300/10000, Train Loss: 0.9524, Val Loss: 1.0582\n",
      "Epoch 2320/10000, Train Loss: 0.9523, Val Loss: 1.0567\n",
      "Epoch 2340/10000, Train Loss: 0.9496, Val Loss: 1.0569\n",
      "Epoch 2360/10000, Train Loss: 0.9485, Val Loss: 1.0540\n",
      "Epoch 2380/10000, Train Loss: 0.9488, Val Loss: 1.0549\n",
      "Epoch 2400/10000, Train Loss: 0.9476, Val Loss: 1.0517\n",
      "Epoch 2420/10000, Train Loss: 0.9465, Val Loss: 1.0560\n",
      "Epoch 2440/10000, Train Loss: 0.9439, Val Loss: 1.0525\n",
      "Epoch 2460/10000, Train Loss: 0.9426, Val Loss: 1.0503\n",
      "Epoch 2480/10000, Train Loss: 0.9417, Val Loss: 1.0490\n",
      "Epoch 2500/10000, Train Loss: 0.9417, Val Loss: 1.0463\n",
      "Epoch 2520/10000, Train Loss: 0.9411, Val Loss: 1.0469\n",
      "Epoch 2540/10000, Train Loss: 0.9387, Val Loss: 1.0489\n",
      "Epoch 2560/10000, Train Loss: 0.9434, Val Loss: 1.0433\n",
      "Epoch 2580/10000, Train Loss: 0.9367, Val Loss: 1.0488\n",
      "Epoch 2600/10000, Train Loss: 0.9387, Val Loss: 1.0410\n",
      "Epoch 2620/10000, Train Loss: 0.9354, Val Loss: 1.0452\n",
      "Epoch 2640/10000, Train Loss: 0.9356, Val Loss: 1.0464\n",
      "Epoch 2660/10000, Train Loss: 0.9342, Val Loss: 1.0429\n",
      "Epoch 2680/10000, Train Loss: 0.9326, Val Loss: 1.0366\n",
      "Epoch 2700/10000, Train Loss: 0.9310, Val Loss: 1.0353\n",
      "Epoch 2720/10000, Train Loss: 0.9295, Val Loss: 1.0342\n",
      "Epoch 2740/10000, Train Loss: 0.9288, Val Loss: 1.0356\n",
      "Epoch 2760/10000, Train Loss: 0.9255, Val Loss: 1.0374\n",
      "Epoch 2780/10000, Train Loss: 0.9272, Val Loss: 1.0318\n",
      "Epoch 2800/10000, Train Loss: 0.9262, Val Loss: 1.0304\n",
      "Epoch 2820/10000, Train Loss: 0.9251, Val Loss: 1.0317\n",
      "Epoch 2840/10000, Train Loss: 0.9238, Val Loss: 1.0291\n",
      "Epoch 2860/10000, Train Loss: 0.9258, Val Loss: 1.0387\n",
      "Epoch 2880/10000, Train Loss: 0.9219, Val Loss: 1.0293\n",
      "Epoch 2900/10000, Train Loss: 0.9236, Val Loss: 1.0366\n",
      "Epoch 2920/10000, Train Loss: 0.9210, Val Loss: 1.0274\n",
      "Epoch 2940/10000, Train Loss: 0.9208, Val Loss: 1.0242\n",
      "Epoch 2960/10000, Train Loss: 0.9192, Val Loss: 1.0249\n",
      "Epoch 2980/10000, Train Loss: 0.9194, Val Loss: 1.0238\n",
      "Epoch 3000/10000, Train Loss: 0.9200, Val Loss: 1.0210\n",
      "Epoch 3020/10000, Train Loss: 0.9165, Val Loss: 1.0224\n",
      "Epoch 3040/10000, Train Loss: 0.9150, Val Loss: 1.0217\n",
      "Epoch 3060/10000, Train Loss: 0.9152, Val Loss: 1.0187\n",
      "Epoch 3080/10000, Train Loss: 0.9172, Val Loss: 1.0193\n",
      "Epoch 3100/10000, Train Loss: 0.9147, Val Loss: 1.0203\n",
      "Epoch 3120/10000, Train Loss: 0.9151, Val Loss: 1.0167\n",
      "Epoch 3140/10000, Train Loss: 0.9123, Val Loss: 1.0204\n",
      "Epoch 3160/10000, Train Loss: 0.9126, Val Loss: 1.0147\n",
      "Epoch 3180/10000, Train Loss: 0.9094, Val Loss: 1.0138\n",
      "Epoch 3200/10000, Train Loss: 0.9107, Val Loss: 1.0139\n",
      "Epoch 3220/10000, Train Loss: 0.9115, Val Loss: 1.0134\n",
      "Epoch 3240/10000, Train Loss: 0.9095, Val Loss: 1.0119\n",
      "Epoch 3260/10000, Train Loss: 0.9075, Val Loss: 1.0133\n",
      "Epoch 3280/10000, Train Loss: 0.9069, Val Loss: 1.0103\n",
      "Epoch 3300/10000, Train Loss: 0.9069, Val Loss: 1.0119\n",
      "Epoch 3320/10000, Train Loss: 0.9055, Val Loss: 1.0085\n",
      "Epoch 3340/10000, Train Loss: 0.9046, Val Loss: 1.0077\n",
      "Epoch 3360/10000, Train Loss: 0.9072, Val Loss: 1.0075\n",
      "Epoch 3380/10000, Train Loss: 0.9038, Val Loss: 1.0084\n",
      "Epoch 3400/10000, Train Loss: 0.9021, Val Loss: 1.0067\n",
      "Epoch 3420/10000, Train Loss: 0.9042, Val Loss: 1.0061\n",
      "Epoch 3440/10000, Train Loss: 0.9008, Val Loss: 1.0045\n",
      "Epoch 3460/10000, Train Loss: 0.9029, Val Loss: 1.0088\n",
      "Epoch 3480/10000, Train Loss: 0.8997, Val Loss: 1.0027\n",
      "Epoch 3500/10000, Train Loss: 0.8978, Val Loss: 1.0028\n",
      "Epoch 3520/10000, Train Loss: 0.8974, Val Loss: 1.0011\n",
      "Epoch 3540/10000, Train Loss: 0.8976, Val Loss: 1.0042\n",
      "Epoch 3560/10000, Train Loss: 0.8964, Val Loss: 1.0014\n",
      "Epoch 3580/10000, Train Loss: 0.8957, Val Loss: 1.0014\n",
      "Epoch 3600/10000, Train Loss: 0.8963, Val Loss: 0.9998\n",
      "Epoch 3620/10000, Train Loss: 0.8948, Val Loss: 0.9994\n",
      "Epoch 3640/10000, Train Loss: 0.8935, Val Loss: 1.0000\n",
      "Epoch 3660/10000, Train Loss: 0.8941, Val Loss: 0.9979\n",
      "Epoch 3680/10000, Train Loss: 0.8926, Val Loss: 0.9968\n",
      "Epoch 3700/10000, Train Loss: 0.8924, Val Loss: 0.9956\n",
      "Epoch 3720/10000, Train Loss: 0.8939, Val Loss: 0.9975\n",
      "Epoch 3740/10000, Train Loss: 0.8924, Val Loss: 0.9941\n",
      "Epoch 3760/10000, Train Loss: 0.8911, Val Loss: 0.9934\n",
      "Epoch 3780/10000, Train Loss: 0.8939, Val Loss: 0.9943\n",
      "Epoch 3800/10000, Train Loss: 0.8907, Val Loss: 0.9931\n",
      "Epoch 3820/10000, Train Loss: 0.8897, Val Loss: 0.9941\n",
      "Epoch 3840/10000, Train Loss: 0.8878, Val Loss: 0.9920\n",
      "Epoch 3860/10000, Train Loss: 0.8885, Val Loss: 0.9918\n",
      "Epoch 3880/10000, Train Loss: 0.8881, Val Loss: 0.9932\n"
     ]
    }
   ],
   "source": [
    "training_feature_tv_split_nor, training_label_tv_split_nor, validating_feature_tv_split_nor, validating_label_tv_split_nor = split_to_train_and_validate(training_feature_normalized, training_label, 0.2)\n",
    "\n",
    "dataset_train_nor = CovidDataset(training_feature_tv_split_nor, training_label_tv_split_nor)\n",
    "dataset_val_nor = CovidDataset(validating_feature_tv_split_nor, validating_label_tv_split_nor)\n",
    "dataloader_train_nor = DataLoader(dataset_train_nor, batch_size=128, shuffle=True)\n",
    "dataloader_val_nor = DataLoader(dataset_val_nor, batch_size=128, shuffle=True)\n",
    "\n",
    "model_d1_tv_split_nor = RegressionModel_d1(input_dim=53)\n",
    "train_model_with_validation(model_d1_tv_split_nor, dataloader_train_nor, dataloader_val_nor, 10000, lr=2e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b774f3b-775f-4abc-b5a0-1efc3586a0f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tested_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>32.540447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.334736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>25.162697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>26.766750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>40.326927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>992</td>\n",
       "      <td>15.866301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>993</td>\n",
       "      <td>13.426210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>994</td>\n",
       "      <td>19.733818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>21.479240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>34.311275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>997 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  tested_positive\n",
       "0      0        32.540447\n",
       "1      1         6.334736\n",
       "2      2        25.162697\n",
       "3      3        26.766750\n",
       "4      4        40.326927\n",
       "..   ...              ...\n",
       "992  992        15.866301\n",
       "993  993        13.426210\n",
       "994  994        19.733818\n",
       "995  995        21.479240\n",
       "996  996        34.311275\n",
       "\n",
       "[997 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d1_tv_split_nor.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, row in testing_feature_normalized.iterrows():\n",
    "        input_tensor = torch.tensor(row.to_numpy(dtype=np.float32), dtype=torch.float32).unsqueeze(0)\n",
    "        output = model_d1_tv_split_nor(input_tensor)\n",
    "        predictions.append(output.item())\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': range(len(predictions)),\n",
    "    'tested_positive': predictions\n",
    "})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337feea7-6c46-47aa-b2a6-87b0e12859c5",
   "metadata": {},
   "source": [
    "## d2 + tv split + Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "344c0d3f-37cc-4723-93a6-2120c6c36430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/20000, Train Loss: 61.4736, Val Loss: 58.8760\n",
      "Epoch 100/20000, Train Loss: 11.4033, Val Loss: 11.0163\n",
      "Epoch 150/20000, Train Loss: 8.5840, Val Loss: 8.1120\n",
      "Epoch 200/20000, Train Loss: 5.9763, Val Loss: 5.5829\n",
      "Epoch 250/20000, Train Loss: 3.9681, Val Loss: 3.7582\n",
      "Epoch 300/20000, Train Loss: 2.7737, Val Loss: 2.7404\n",
      "Epoch 350/20000, Train Loss: 2.1435, Val Loss: 2.2151\n",
      "Epoch 400/20000, Train Loss: 1.7771, Val Loss: 1.8778\n",
      "Epoch 450/20000, Train Loss: 1.5436, Val Loss: 1.6317\n",
      "Epoch 500/20000, Train Loss: 1.3745, Val Loss: 1.4602\n",
      "Epoch 550/20000, Train Loss: 1.2659, Val Loss: 1.3451\n",
      "Epoch 600/20000, Train Loss: 1.1977, Val Loss: 1.2726\n",
      "Epoch 650/20000, Train Loss: 1.1531, Val Loss: 1.2168\n",
      "Epoch 700/20000, Train Loss: 1.1142, Val Loss: 1.1963\n",
      "Epoch 750/20000, Train Loss: 1.0809, Val Loss: 1.1407\n",
      "Epoch 800/20000, Train Loss: 1.0566, Val Loss: 1.1233\n",
      "Epoch 850/20000, Train Loss: 1.0401, Val Loss: 1.1014\n",
      "Epoch 900/20000, Train Loss: 1.0240, Val Loss: 1.0838\n",
      "Epoch 950/20000, Train Loss: 1.0122, Val Loss: 1.0662\n",
      "Epoch 1000/20000, Train Loss: 1.0201, Val Loss: 1.0554\n",
      "Epoch 1050/20000, Train Loss: 0.9928, Val Loss: 1.0438\n",
      "Epoch 1100/20000, Train Loss: 0.9789, Val Loss: 1.0343\n",
      "Epoch 1150/20000, Train Loss: 0.9818, Val Loss: 1.0304\n",
      "Epoch 1200/20000, Train Loss: 0.9702, Val Loss: 1.0338\n",
      "Epoch 1250/20000, Train Loss: 0.9583, Val Loss: 1.0075\n",
      "Epoch 1300/20000, Train Loss: 0.9531, Val Loss: 1.0008\n",
      "Epoch 1350/20000, Train Loss: 0.9474, Val Loss: 1.0236\n",
      "Epoch 1400/20000, Train Loss: 0.9358, Val Loss: 1.0132\n",
      "Epoch 1450/20000, Train Loss: 0.9317, Val Loss: 0.9884\n",
      "Epoch 1500/20000, Train Loss: 0.9242, Val Loss: 0.9818\n",
      "Epoch 1550/20000, Train Loss: 0.9229, Val Loss: 0.9785\n",
      "Epoch 1600/20000, Train Loss: 0.9181, Val Loss: 0.9756\n",
      "Epoch 1650/20000, Train Loss: 0.9097, Val Loss: 0.9731\n",
      "Epoch 1700/20000, Train Loss: 0.9167, Val Loss: 0.9700\n",
      "Epoch 1750/20000, Train Loss: 0.9011, Val Loss: 0.9891\n",
      "Epoch 1800/20000, Train Loss: 0.8966, Val Loss: 0.9646\n",
      "Epoch 1850/20000, Train Loss: 0.8992, Val Loss: 0.9591\n",
      "Epoch 1900/20000, Train Loss: 0.8887, Val Loss: 0.9540\n",
      "Epoch 1950/20000, Train Loss: 0.8889, Val Loss: 0.9681\n",
      "Epoch 2000/20000, Train Loss: 0.8816, Val Loss: 0.9549\n",
      "Epoch 2050/20000, Train Loss: 0.8778, Val Loss: 0.9447\n",
      "Epoch 2100/20000, Train Loss: 0.8770, Val Loss: 0.9691\n",
      "Epoch 2150/20000, Train Loss: 0.8714, Val Loss: 0.9423\n",
      "Epoch 2200/20000, Train Loss: 0.8692, Val Loss: 0.9401\n",
      "Epoch 2250/20000, Train Loss: 0.8682, Val Loss: 0.9412\n",
      "Epoch 2300/20000, Train Loss: 0.8634, Val Loss: 0.9409\n",
      "Epoch 2350/20000, Train Loss: 0.8628, Val Loss: 0.9391\n",
      "Epoch 2400/20000, Train Loss: 0.8589, Val Loss: 0.9439\n",
      "Epoch 2450/20000, Train Loss: 0.8565, Val Loss: 0.9311\n",
      "Epoch 2500/20000, Train Loss: 0.8540, Val Loss: 0.9369\n",
      "Epoch 2550/20000, Train Loss: 0.8571, Val Loss: 0.9338\n",
      "Epoch 2600/20000, Train Loss: 0.8495, Val Loss: 0.9377\n",
      "Epoch 2650/20000, Train Loss: 0.8484, Val Loss: 0.9394\n",
      "Epoch 2700/20000, Train Loss: 0.8493, Val Loss: 0.9286\n",
      "Epoch 2750/20000, Train Loss: 0.8449, Val Loss: 0.9501\n",
      "Epoch 2800/20000, Train Loss: 0.8453, Val Loss: 0.9346\n",
      "Epoch 2850/20000, Train Loss: 0.8527, Val Loss: 0.9282\n",
      "Epoch 2900/20000, Train Loss: 0.8420, Val Loss: 0.9291\n",
      "Epoch 2950/20000, Train Loss: 0.8415, Val Loss: 0.9469\n",
      "Epoch 3000/20000, Train Loss: 0.8539, Val Loss: 0.9242\n",
      "Epoch 3050/20000, Train Loss: 0.8424, Val Loss: 0.9276\n",
      "Epoch 3100/20000, Train Loss: 0.8368, Val Loss: 0.9351\n",
      "Epoch 3150/20000, Train Loss: 0.8393, Val Loss: 0.9238\n",
      "Epoch 3200/20000, Train Loss: 0.8378, Val Loss: 0.9353\n",
      "Epoch 3250/20000, Train Loss: 0.8352, Val Loss: 0.9259\n",
      "Epoch 3300/20000, Train Loss: 0.8362, Val Loss: 0.9242\n",
      "Epoch 3350/20000, Train Loss: 0.8339, Val Loss: 0.9233\n",
      "Epoch 3400/20000, Train Loss: 0.8359, Val Loss: 0.9322\n",
      "Epoch 3450/20000, Train Loss: 0.8340, Val Loss: 0.9254\n",
      "Epoch 3500/20000, Train Loss: 0.8373, Val Loss: 0.9283\n",
      "Epoch 3550/20000, Train Loss: 0.8383, Val Loss: 0.9245\n",
      "Epoch 3600/20000, Train Loss: 0.8325, Val Loss: 0.9242\n",
      "Epoch 3650/20000, Train Loss: 0.8357, Val Loss: 0.9248\n",
      "Epoch 3700/20000, Train Loss: 0.8353, Val Loss: 0.9249\n",
      "Epoch 3750/20000, Train Loss: 0.8369, Val Loss: 0.9338\n"
     ]
    }
   ],
   "source": [
    "training_feature_tv_split_nor, training_label_tv_split_nor, validating_feature_tv_split_nor, validating_label_tv_split_nor = split_to_train_and_validate(training_feature_normalized, training_label, 0.2)\n",
    "\n",
    "dataset_train_nor = CovidDataset(training_feature_tv_split_nor, training_label_tv_split_nor)\n",
    "dataset_val_nor = CovidDataset(validating_feature_tv_split_nor, validating_label_tv_split_nor)\n",
    "dataloader_train_nor = DataLoader(dataset_train_nor, batch_size=256, shuffle=True)\n",
    "dataloader_val_nor = DataLoader(dataset_val_nor, batch_size=256, shuffle=True)\n",
    "\n",
    "\n",
    "model_d3_tv_split_nor = RegressionModel_d3(input_dim=53, h1=16, h2=8)\n",
    "train_model_with_validation(model_d3_tv_split_nor, dataloader_train_nor, dataloader_val_nor, 20000, lr=4e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "ec1c9c66-989d-4c6e-a106-498d1554313b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tested_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>32.221394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6.068198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>25.001127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>26.644464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>40.235355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>992</td>\n",
       "      <td>15.891931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>993</td>\n",
       "      <td>13.157825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>994</td>\n",
       "      <td>19.787922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>21.163931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>34.275715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>997 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  tested_positive\n",
       "0      0        32.221394\n",
       "1      1         6.068198\n",
       "2      2        25.001127\n",
       "3      3        26.644464\n",
       "4      4        40.235355\n",
       "..   ...              ...\n",
       "992  992        15.891931\n",
       "993  993        13.157825\n",
       "994  994        19.787922\n",
       "995  995        21.163931\n",
       "996  996        34.275715\n",
       "\n",
       "[997 rows x 2 columns]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_d1_tv_split_nor.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, row in testing_feature_normalized.iterrows():\n",
    "        input_tensor = torch.tensor(row.to_numpy(dtype=np.float32), dtype=torch.float32).unsqueeze(0)\n",
    "        output = model_d3_tv_split_nor(input_tensor)\n",
    "        predictions.append(output.item())\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': range(len(predictions)),\n",
    "    'tested_positive': predictions\n",
    "})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc141f6-7197-426c-9bc1-dd367fae58e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a926d-3a40-4e12-950d-57f433ae5bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
